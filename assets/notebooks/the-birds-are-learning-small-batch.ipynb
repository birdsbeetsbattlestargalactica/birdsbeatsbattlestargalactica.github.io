{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f03eaa4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-06-04T21:11:01.920893Z",
     "iopub.status.busy": "2023-06-04T21:11:01.920544Z",
     "iopub.status.idle": "2023-06-05T00:31:40.627932Z",
     "shell.execute_reply": "2023-06-05T00:31:40.626506Z"
    },
    "papermill": {
     "duration": 12038.718986,
     "end_time": "2023-06-05T00:31:40.636012",
     "exception": false,
     "start_time": "2023-06-04T21:11:01.917026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/v0.6.0\" to /root/.cache/torch/hub/v0.6.0.zip\n",
      "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
      "100%|██████████| 83.3M/83.3M [00:00<00:00, 226MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.010000\n",
      "[0,    10] loss: 6.536\n",
      "[0,    20] loss: 6.488\n",
      "[0,    30] loss: 6.356\n",
      "[0,    40] loss: 6.142\n",
      "[0,    50] loss: 6.060\n",
      "[0,    60] loss: 5.841\n",
      "[0,    70] loss: 5.759\n",
      "[0,    80] loss: 5.660\n",
      "[0,    90] loss: 5.425\n",
      "[0,   100] loss: 5.339\n",
      "[0,   110] loss: 5.192\n",
      "[0,   120] loss: 5.120\n",
      "[0,   130] loss: 4.797\n",
      "[0,   140] loss: 4.931\n",
      "[0,   150] loss: 4.799\n",
      "[0,   160] loss: 4.582\n",
      "[0,   170] loss: 4.499\n",
      "[0,   180] loss: 4.262\n",
      "[0,   190] loss: 4.273\n",
      "[0,   200] loss: 4.510\n",
      "[0,   210] loss: 4.267\n",
      "[0,   220] loss: 4.024\n",
      "[0,   230] loss: 3.935\n",
      "[0,   240] loss: 3.997\n",
      "[0,   250] loss: 3.980\n",
      "[0,   260] loss: 3.904\n",
      "[0,   270] loss: 3.780\n",
      "[0,   280] loss: 3.685\n",
      "[0,   290] loss: 3.821\n",
      "[0,   300] loss: 3.656\n",
      "[0,   310] loss: 3.680\n",
      "[0,   320] loss: 3.667\n",
      "[0,   330] loss: 3.637\n",
      "[0,   340] loss: 3.564\n",
      "[0,   350] loss: 3.409\n",
      "[0,   360] loss: 3.238\n",
      "[0,   370] loss: 3.472\n",
      "[0,   380] loss: 3.302\n",
      "[0,   390] loss: 3.292\n",
      "[0,   400] loss: 3.165\n",
      "[0,   410] loss: 3.292\n",
      "[0,   420] loss: 3.286\n",
      "[0,   430] loss: 3.099\n",
      "[0,   440] loss: 3.050\n",
      "[0,   450] loss: 3.175\n",
      "[0,   460] loss: 3.160\n",
      "[0,   470] loss: 3.118\n",
      "[0,   480] loss: 2.966\n",
      "[0,   490] loss: 3.017\n",
      "[0,   500] loss: 3.107\n",
      "[0,   510] loss: 2.863\n",
      "[0,   520] loss: 3.043\n",
      "[0,   530] loss: 2.966\n",
      "[0,   540] loss: 3.141\n",
      "[0,   550] loss: 2.882\n",
      "[0,   560] loss: 2.889\n",
      "[0,   570] loss: 2.952\n",
      "[0,   580] loss: 2.844\n",
      "[0,   590] loss: 2.960\n",
      "[0,   600] loss: 2.745\n",
      "[0,   610] loss: 2.929\n",
      "[0,   620] loss: 2.982\n",
      "[0,   630] loss: 2.820\n",
      "[0,   640] loss: 3.014\n",
      "[0,   650] loss: 2.811\n",
      "[0,   660] loss: 2.845\n",
      "[0,   670] loss: 2.886\n",
      "[0,   680] loss: 2.721\n",
      "[0,   690] loss: 2.777\n",
      "[0,   700] loss: 2.587\n",
      "[0,   710] loss: 2.792\n",
      "[0,   720] loss: 2.635\n",
      "[0,   730] loss: 2.752\n",
      "[0,   740] loss: 2.818\n",
      "[0,   750] loss: 2.653\n",
      "[0,   760] loss: 2.484\n",
      "[0,   770] loss: 2.445\n",
      "[0,   780] loss: 2.587\n",
      "[0,   790] loss: 2.525\n",
      "[0,   800] loss: 2.741\n",
      "[0,   810] loss: 2.526\n",
      "[0,   820] loss: 2.565\n",
      "[0,   830] loss: 2.620\n",
      "[0,   840] loss: 2.404\n",
      "[0,   850] loss: 2.414\n",
      "[0,   860] loss: 2.664\n",
      "[0,   870] loss: 2.357\n",
      "[0,   880] loss: 2.367\n",
      "[0,   890] loss: 2.455\n",
      "[0,   900] loss: 2.295\n",
      "[0,   910] loss: 2.496\n",
      "[0,   920] loss: 2.468\n",
      "[0,   930] loss: 2.380\n",
      "[0,   940] loss: 2.433\n",
      "[0,   950] loss: 2.461\n",
      "[0,   960] loss: 2.297\n",
      "[0,   970] loss: 2.291\n",
      "[0,   980] loss: 2.210\n",
      "[0,   990] loss: 2.365\n",
      "[0,  1000] loss: 2.236\n",
      "[0,  1010] loss: 2.397\n",
      "[0,  1020] loss: 2.351\n",
      "[0,  1030] loss: 2.247\n",
      "[0,  1040] loss: 2.379\n",
      "[0,  1050] loss: 2.161\n",
      "[0,  1060] loss: 2.242\n",
      "[0,  1070] loss: 2.175\n",
      "[0,  1080] loss: 2.202\n",
      "[0,  1090] loss: 2.196\n",
      "[0,  1100] loss: 2.218\n",
      "[0,  1110] loss: 2.262\n",
      "[0,  1120] loss: 2.414\n",
      "[0,  1130] loss: 2.279\n",
      "[0,  1140] loss: 2.268\n",
      "[0,  1150] loss: 2.081\n",
      "[0,  1160] loss: 2.372\n",
      "[0,  1170] loss: 2.165\n",
      "[0,  1180] loss: 2.201\n",
      "[0,  1190] loss: 2.290\n",
      "[0,  1200] loss: 2.199\n",
      "Learning rate: 0.010000\n",
      "[1,    10] loss: 2.194\n",
      "[1,    20] loss: 1.874\n",
      "[1,    30] loss: 1.921\n",
      "[1,    40] loss: 2.010\n",
      "[1,    50] loss: 2.041\n",
      "[1,    60] loss: 2.042\n",
      "[1,    70] loss: 1.842\n",
      "[1,    80] loss: 1.760\n",
      "[1,    90] loss: 2.083\n",
      "[1,   100] loss: 1.816\n",
      "[1,   110] loss: 1.859\n",
      "[1,   120] loss: 1.959\n",
      "[1,   130] loss: 1.899\n",
      "[1,   140] loss: 1.914\n",
      "[1,   150] loss: 1.829\n",
      "[1,   160] loss: 1.970\n",
      "[1,   170] loss: 1.949\n",
      "[1,   180] loss: 1.961\n",
      "[1,   190] loss: 1.914\n",
      "[1,   200] loss: 2.089\n",
      "[1,   210] loss: 1.878\n",
      "[1,   220] loss: 1.779\n",
      "[1,   230] loss: 2.137\n",
      "[1,   240] loss: 1.946\n",
      "[1,   250] loss: 1.915\n",
      "[1,   260] loss: 1.989\n",
      "[1,   270] loss: 1.868\n",
      "[1,   280] loss: 1.837\n",
      "[1,   290] loss: 1.869\n",
      "[1,   300] loss: 1.712\n",
      "[1,   310] loss: 2.052\n",
      "[1,   320] loss: 2.074\n",
      "[1,   330] loss: 1.901\n",
      "[1,   340] loss: 1.932\n",
      "[1,   350] loss: 1.750\n",
      "[1,   360] loss: 1.736\n",
      "[1,   370] loss: 1.917\n",
      "[1,   380] loss: 1.866\n",
      "[1,   390] loss: 1.998\n",
      "[1,   400] loss: 1.891\n",
      "[1,   410] loss: 1.949\n",
      "[1,   420] loss: 1.902\n",
      "[1,   430] loss: 1.863\n",
      "[1,   440] loss: 2.094\n",
      "[1,   450] loss: 1.970\n",
      "[1,   460] loss: 1.871\n",
      "[1,   470] loss: 1.929\n",
      "[1,   480] loss: 1.904\n",
      "[1,   490] loss: 2.038\n",
      "[1,   500] loss: 1.846\n",
      "[1,   510] loss: 2.007\n",
      "[1,   520] loss: 1.861\n",
      "[1,   530] loss: 1.949\n",
      "[1,   540] loss: 1.937\n",
      "[1,   550] loss: 2.033\n",
      "[1,   560] loss: 1.858\n",
      "[1,   570] loss: 1.740\n",
      "[1,   580] loss: 1.905\n",
      "[1,   590] loss: 1.988\n",
      "[1,   600] loss: 1.765\n",
      "[1,   610] loss: 1.882\n",
      "[1,   620] loss: 1.830\n",
      "[1,   630] loss: 1.779\n",
      "[1,   640] loss: 1.787\n",
      "[1,   650] loss: 1.855\n",
      "[1,   660] loss: 1.882\n",
      "[1,   670] loss: 1.807\n",
      "[1,   680] loss: 1.724\n",
      "[1,   690] loss: 1.793\n",
      "[1,   700] loss: 1.924\n",
      "[1,   710] loss: 1.784\n",
      "[1,   720] loss: 1.880\n",
      "[1,   730] loss: 1.967\n",
      "[1,   740] loss: 1.797\n",
      "[1,   750] loss: 1.862\n",
      "[1,   760] loss: 1.729\n",
      "[1,   770] loss: 1.658\n",
      "[1,   780] loss: 1.804\n",
      "[1,   790] loss: 1.865\n",
      "[1,   800] loss: 1.601\n",
      "[1,   810] loss: 1.834\n",
      "[1,   820] loss: 1.716\n",
      "[1,   830] loss: 1.680\n",
      "[1,   840] loss: 1.997\n",
      "[1,   850] loss: 1.765\n",
      "[1,   860] loss: 1.808\n",
      "[1,   870] loss: 1.731\n",
      "[1,   880] loss: 1.752\n",
      "[1,   890] loss: 1.618\n",
      "[1,   900] loss: 1.715\n",
      "[1,   910] loss: 1.571\n",
      "[1,   920] loss: 1.876\n",
      "[1,   930] loss: 1.639\n",
      "[1,   940] loss: 1.640\n",
      "[1,   950] loss: 1.649\n",
      "[1,   960] loss: 1.704\n",
      "[1,   970] loss: 1.819\n",
      "[1,   980] loss: 1.757\n",
      "[1,   990] loss: 1.795\n",
      "[1,  1000] loss: 1.743\n",
      "[1,  1010] loss: 1.842\n",
      "[1,  1020] loss: 1.644\n",
      "[1,  1030] loss: 1.936\n",
      "[1,  1040] loss: 1.683\n",
      "[1,  1050] loss: 1.670\n",
      "[1,  1060] loss: 1.736\n",
      "[1,  1070] loss: 1.776\n",
      "[1,  1080] loss: 1.621\n",
      "[1,  1090] loss: 1.715\n",
      "[1,  1100] loss: 1.954\n",
      "[1,  1110] loss: 1.812\n",
      "[1,  1120] loss: 1.833\n",
      "[1,  1130] loss: 1.715\n",
      "[1,  1140] loss: 1.779\n",
      "[1,  1150] loss: 1.570\n",
      "[1,  1160] loss: 1.646\n",
      "[1,  1170] loss: 1.708\n",
      "[1,  1180] loss: 1.583\n",
      "[1,  1190] loss: 1.617\n",
      "[1,  1200] loss: 1.739\n",
      "Learning rate: 0.010000\n",
      "[2,    10] loss: 1.668\n",
      "[2,    20] loss: 1.567\n",
      "[2,    30] loss: 1.338\n",
      "[2,    40] loss: 1.368\n",
      "[2,    50] loss: 1.417\n",
      "[2,    60] loss: 1.474\n",
      "[2,    70] loss: 1.482\n",
      "[2,    80] loss: 1.434\n",
      "[2,    90] loss: 1.491\n",
      "[2,   100] loss: 1.537\n",
      "[2,   110] loss: 1.270\n",
      "[2,   120] loss: 1.599\n",
      "[2,   130] loss: 1.528\n",
      "[2,   140] loss: 1.581\n",
      "[2,   150] loss: 1.542\n",
      "[2,   160] loss: 1.355\n",
      "[2,   170] loss: 1.366\n",
      "[2,   180] loss: 1.603\n",
      "[2,   190] loss: 1.555\n",
      "[2,   200] loss: 1.470\n",
      "[2,   210] loss: 1.305\n",
      "[2,   220] loss: 1.403\n",
      "[2,   230] loss: 1.382\n",
      "[2,   240] loss: 1.447\n",
      "[2,   250] loss: 1.258\n",
      "[2,   260] loss: 1.420\n",
      "[2,   270] loss: 1.389\n",
      "[2,   280] loss: 1.430\n",
      "[2,   290] loss: 1.517\n",
      "[2,   300] loss: 1.525\n",
      "[2,   310] loss: 1.274\n",
      "[2,   320] loss: 1.455\n",
      "[2,   330] loss: 1.617\n",
      "[2,   340] loss: 1.633\n",
      "[2,   350] loss: 1.378\n",
      "[2,   360] loss: 1.396\n",
      "[2,   370] loss: 1.550\n",
      "[2,   380] loss: 1.341\n",
      "[2,   390] loss: 1.511\n",
      "[2,   400] loss: 1.352\n",
      "[2,   410] loss: 1.362\n",
      "[2,   420] loss: 1.566\n",
      "[2,   430] loss: 1.503\n",
      "[2,   440] loss: 1.508\n",
      "[2,   450] loss: 1.428\n",
      "[2,   460] loss: 1.468\n",
      "[2,   470] loss: 1.429\n",
      "[2,   480] loss: 1.359\n",
      "[2,   490] loss: 1.439\n",
      "[2,   500] loss: 1.459\n",
      "[2,   510] loss: 1.622\n",
      "[2,   520] loss: 1.601\n",
      "[2,   530] loss: 1.616\n",
      "[2,   540] loss: 1.380\n",
      "[2,   550] loss: 1.467\n",
      "[2,   560] loss: 1.485\n",
      "[2,   570] loss: 1.376\n",
      "[2,   580] loss: 1.416\n",
      "[2,   590] loss: 1.354\n",
      "[2,   600] loss: 1.513\n",
      "[2,   610] loss: 1.670\n",
      "[2,   620] loss: 1.508\n",
      "[2,   630] loss: 1.597\n",
      "[2,   640] loss: 1.562\n",
      "[2,   650] loss: 1.584\n",
      "[2,   660] loss: 1.487\n",
      "[2,   670] loss: 1.396\n",
      "[2,   680] loss: 1.442\n",
      "[2,   690] loss: 1.389\n",
      "[2,   700] loss: 1.404\n",
      "[2,   710] loss: 1.322\n",
      "[2,   720] loss: 1.374\n",
      "[2,   730] loss: 1.481\n",
      "[2,   740] loss: 1.406\n",
      "[2,   750] loss: 1.302\n",
      "[2,   760] loss: 1.521\n",
      "[2,   770] loss: 1.320\n",
      "[2,   780] loss: 1.528\n",
      "[2,   790] loss: 1.511\n",
      "[2,   800] loss: 1.318\n",
      "[2,   810] loss: 1.489\n",
      "[2,   820] loss: 1.459\n",
      "[2,   830] loss: 1.349\n",
      "[2,   840] loss: 1.293\n",
      "[2,   850] loss: 1.495\n",
      "[2,   860] loss: 1.418\n",
      "[2,   870] loss: 1.462\n",
      "[2,   880] loss: 1.445\n",
      "[2,   890] loss: 1.372\n",
      "[2,   900] loss: 1.400\n",
      "[2,   910] loss: 1.449\n",
      "[2,   920] loss: 1.370\n",
      "[2,   930] loss: 1.484\n",
      "[2,   940] loss: 1.435\n",
      "[2,   950] loss: 1.454\n",
      "[2,   960] loss: 1.397\n",
      "[2,   970] loss: 1.234\n",
      "[2,   980] loss: 1.390\n",
      "[2,   990] loss: 1.481\n",
      "[2,  1000] loss: 1.461\n",
      "[2,  1010] loss: 1.374\n",
      "[2,  1020] loss: 1.452\n",
      "[2,  1030] loss: 1.439\n",
      "[2,  1040] loss: 1.580\n",
      "[2,  1050] loss: 1.454\n",
      "[2,  1060] loss: 1.497\n",
      "[2,  1070] loss: 1.450\n",
      "[2,  1080] loss: 1.423\n",
      "[2,  1090] loss: 1.492\n",
      "[2,  1100] loss: 1.563\n",
      "[2,  1110] loss: 1.291\n",
      "[2,  1120] loss: 1.388\n",
      "[2,  1130] loss: 1.371\n",
      "[2,  1140] loss: 1.460\n",
      "[2,  1150] loss: 1.587\n",
      "[2,  1160] loss: 1.439\n",
      "[2,  1170] loss: 1.493\n",
      "[2,  1180] loss: 1.648\n",
      "[2,  1190] loss: 1.251\n",
      "[2,  1200] loss: 1.523\n",
      "Learning rate: 0.010000\n",
      "[3,    10] loss: 1.276\n",
      "[3,    20] loss: 1.378\n",
      "[3,    30] loss: 1.135\n",
      "[3,    40] loss: 1.311\n",
      "[3,    50] loss: 1.319\n",
      "[3,    60] loss: 1.249\n",
      "[3,    70] loss: 1.247\n",
      "[3,    80] loss: 1.183\n",
      "[3,    90] loss: 1.273\n",
      "[3,   100] loss: 1.252\n",
      "[3,   110] loss: 1.202\n",
      "[3,   120] loss: 1.127\n",
      "[3,   130] loss: 1.169\n",
      "[3,   140] loss: 1.073\n",
      "[3,   150] loss: 1.150\n",
      "[3,   160] loss: 1.274\n",
      "[3,   170] loss: 1.295\n",
      "[3,   180] loss: 1.284\n",
      "[3,   190] loss: 1.242\n",
      "[3,   200] loss: 1.306\n",
      "[3,   210] loss: 1.294\n",
      "[3,   220] loss: 1.291\n",
      "[3,   230] loss: 1.143\n",
      "[3,   240] loss: 1.293\n",
      "[3,   250] loss: 1.238\n",
      "[3,   260] loss: 1.092\n",
      "[3,   270] loss: 1.189\n",
      "[3,   280] loss: 1.145\n",
      "[3,   290] loss: 1.243\n",
      "[3,   300] loss: 1.298\n",
      "[3,   310] loss: 1.256\n",
      "[3,   320] loss: 1.155\n",
      "[3,   330] loss: 1.210\n",
      "[3,   340] loss: 1.289\n",
      "[3,   350] loss: 1.185\n",
      "[3,   360] loss: 1.288\n",
      "[3,   370] loss: 1.275\n",
      "[3,   380] loss: 1.177\n",
      "[3,   390] loss: 1.331\n",
      "[3,   400] loss: 1.236\n",
      "[3,   410] loss: 1.202\n",
      "[3,   420] loss: 1.191\n",
      "[3,   430] loss: 1.426\n",
      "[3,   440] loss: 1.202\n",
      "[3,   450] loss: 1.292\n",
      "[3,   460] loss: 1.178\n",
      "[3,   470] loss: 1.258\n",
      "[3,   480] loss: 1.280\n",
      "[3,   490] loss: 1.192\n",
      "[3,   500] loss: 1.226\n",
      "[3,   510] loss: 1.276\n",
      "[3,   520] loss: 1.189\n",
      "[3,   530] loss: 1.332\n",
      "[3,   540] loss: 1.334\n",
      "[3,   550] loss: 1.366\n",
      "[3,   560] loss: 1.340\n",
      "[3,   570] loss: 1.277\n",
      "[3,   580] loss: 1.310\n",
      "[3,   590] loss: 1.333\n",
      "[3,   600] loss: 1.324\n",
      "[3,   610] loss: 1.232\n",
      "[3,   620] loss: 1.105\n",
      "[3,   630] loss: 1.267\n",
      "[3,   640] loss: 1.182\n",
      "[3,   650] loss: 1.281\n",
      "[3,   660] loss: 1.292\n",
      "[3,   670] loss: 1.275\n",
      "[3,   680] loss: 1.225\n",
      "[3,   690] loss: 1.205\n",
      "[3,   700] loss: 1.404\n",
      "[3,   710] loss: 1.469\n",
      "[3,   720] loss: 1.414\n",
      "[3,   730] loss: 1.406\n",
      "[3,   740] loss: 1.405\n",
      "[3,   750] loss: 1.295\n",
      "[3,   760] loss: 1.338\n",
      "[3,   770] loss: 1.088\n",
      "[3,   780] loss: 1.264\n",
      "[3,   790] loss: 1.326\n",
      "[3,   800] loss: 1.338\n",
      "[3,   810] loss: 1.443\n",
      "[3,   820] loss: 1.365\n",
      "[3,   830] loss: 1.200\n",
      "[3,   840] loss: 1.201\n",
      "[3,   850] loss: 1.262\n",
      "[3,   860] loss: 1.255\n",
      "[3,   870] loss: 1.253\n",
      "[3,   880] loss: 1.269\n",
      "[3,   890] loss: 1.307\n",
      "[3,   900] loss: 1.241\n",
      "[3,   910] loss: 1.206\n",
      "[3,   920] loss: 1.146\n",
      "[3,   930] loss: 1.183\n",
      "[3,   940] loss: 1.182\n",
      "[3,   950] loss: 1.362\n",
      "[3,   960] loss: 1.250\n",
      "[3,   970] loss: 1.244\n",
      "[3,   980] loss: 1.211\n",
      "[3,   990] loss: 1.160\n",
      "[3,  1000] loss: 1.123\n",
      "[3,  1010] loss: 1.352\n",
      "[3,  1020] loss: 1.271\n",
      "[3,  1030] loss: 1.465\n",
      "[3,  1040] loss: 1.256\n",
      "[3,  1050] loss: 1.087\n",
      "[3,  1060] loss: 1.073\n",
      "[3,  1070] loss: 1.339\n",
      "[3,  1080] loss: 1.241\n",
      "[3,  1090] loss: 1.306\n",
      "[3,  1100] loss: 1.190\n",
      "[3,  1110] loss: 1.186\n",
      "[3,  1120] loss: 1.195\n",
      "[3,  1130] loss: 1.289\n",
      "[3,  1140] loss: 1.177\n",
      "[3,  1150] loss: 1.140\n",
      "[3,  1160] loss: 1.367\n",
      "[3,  1170] loss: 1.250\n",
      "[3,  1180] loss: 1.232\n",
      "[3,  1190] loss: 1.409\n",
      "[3,  1200] loss: 1.336\n",
      "Learning rate: 0.010000\n",
      "[4,    10] loss: 1.343\n",
      "[4,    20] loss: 1.391\n",
      "[4,    30] loss: 1.093\n",
      "[4,    40] loss: 1.233\n",
      "[4,    50] loss: 1.012\n",
      "[4,    60] loss: 1.193\n",
      "[4,    70] loss: 1.223\n",
      "[4,    80] loss: 1.071\n",
      "[4,    90] loss: 1.008\n",
      "[4,   100] loss: 0.975\n",
      "[4,   110] loss: 1.181\n",
      "[4,   120] loss: 1.079\n",
      "[4,   130] loss: 1.079\n",
      "[4,   140] loss: 1.003\n",
      "[4,   150] loss: 1.012\n",
      "[4,   160] loss: 1.021\n",
      "[4,   170] loss: 1.015\n",
      "[4,   180] loss: 1.061\n",
      "[4,   190] loss: 1.059\n",
      "[4,   200] loss: 1.070\n",
      "[4,   210] loss: 1.005\n",
      "[4,   220] loss: 1.158\n",
      "[4,   230] loss: 1.000\n",
      "[4,   240] loss: 1.131\n",
      "[4,   250] loss: 1.082\n",
      "[4,   260] loss: 0.871\n",
      "[4,   270] loss: 0.969\n",
      "[4,   280] loss: 1.125\n",
      "[4,   290] loss: 1.247\n",
      "[4,   300] loss: 1.148\n",
      "[4,   310] loss: 1.092\n",
      "[4,   320] loss: 1.167\n",
      "[4,   330] loss: 1.035\n",
      "[4,   340] loss: 1.150\n",
      "[4,   350] loss: 1.212\n",
      "[4,   360] loss: 1.047\n",
      "[4,   370] loss: 1.023\n",
      "[4,   380] loss: 1.049\n",
      "[4,   390] loss: 1.140\n",
      "[4,   400] loss: 0.998\n",
      "[4,   410] loss: 1.005\n",
      "[4,   420] loss: 1.188\n",
      "[4,   430] loss: 1.103\n",
      "[4,   440] loss: 1.185\n",
      "[4,   450] loss: 1.140\n",
      "[4,   460] loss: 1.080\n",
      "[4,   470] loss: 1.030\n",
      "[4,   480] loss: 0.996\n",
      "[4,   490] loss: 1.018\n",
      "[4,   500] loss: 0.964\n",
      "[4,   510] loss: 1.024\n",
      "[4,   520] loss: 0.986\n",
      "[4,   530] loss: 1.160\n",
      "[4,   540] loss: 0.985\n",
      "[4,   550] loss: 1.090\n",
      "[4,   560] loss: 1.116\n",
      "[4,   570] loss: 1.069\n",
      "[4,   580] loss: 1.092\n",
      "[4,   590] loss: 1.121\n",
      "[4,   600] loss: 1.211\n",
      "[4,   610] loss: 1.276\n",
      "[4,   620] loss: 1.186\n",
      "[4,   630] loss: 1.189\n",
      "[4,   640] loss: 1.289\n",
      "[4,   650] loss: 1.092\n",
      "[4,   660] loss: 1.185\n",
      "[4,   670] loss: 1.188\n",
      "[4,   680] loss: 1.094\n",
      "[4,   690] loss: 1.150\n",
      "[4,   700] loss: 1.144\n",
      "[4,   710] loss: 1.187\n",
      "[4,   720] loss: 1.014\n",
      "[4,   730] loss: 1.100\n",
      "[4,   740] loss: 1.217\n",
      "[4,   750] loss: 1.043\n",
      "[4,   760] loss: 1.005\n",
      "[4,   770] loss: 1.032\n",
      "[4,   780] loss: 0.992\n",
      "[4,   790] loss: 1.177\n",
      "[4,   800] loss: 1.156\n",
      "[4,   810] loss: 1.199\n",
      "[4,   820] loss: 1.161\n",
      "[4,   830] loss: 1.193\n",
      "[4,   840] loss: 1.197\n",
      "[4,   850] loss: 1.143\n",
      "[4,   860] loss: 1.233\n",
      "[4,   870] loss: 1.292\n",
      "[4,   880] loss: 1.267\n",
      "[4,   890] loss: 1.093\n",
      "[4,   900] loss: 1.103\n",
      "[4,   910] loss: 1.207\n",
      "[4,   920] loss: 1.096\n",
      "[4,   930] loss: 1.245\n",
      "[4,   940] loss: 1.203\n",
      "[4,   950] loss: 1.121\n",
      "[4,   960] loss: 1.104\n",
      "[4,   970] loss: 1.185\n",
      "[4,   980] loss: 1.050\n",
      "[4,   990] loss: 1.179\n",
      "[4,  1000] loss: 0.996\n",
      "[4,  1010] loss: 1.196\n",
      "[4,  1020] loss: 1.017\n",
      "[4,  1030] loss: 1.096\n",
      "[4,  1040] loss: 1.146\n",
      "[4,  1050] loss: 1.045\n",
      "[4,  1060] loss: 1.103\n",
      "[4,  1070] loss: 1.178\n",
      "[4,  1080] loss: 1.102\n",
      "[4,  1090] loss: 1.160\n",
      "[4,  1100] loss: 1.181\n",
      "[4,  1110] loss: 1.123\n",
      "[4,  1120] loss: 1.119\n",
      "[4,  1130] loss: 1.142\n",
      "[4,  1140] loss: 1.030\n",
      "[4,  1150] loss: 1.078\n",
      "[4,  1160] loss: 1.037\n",
      "[4,  1170] loss: 1.320\n",
      "[4,  1180] loss: 1.124\n",
      "[4,  1190] loss: 1.217\n",
      "[4,  1200] loss: 1.098\n",
      "Learning rate: 0.010000\n",
      "[5,    10] loss: 1.153\n",
      "[5,    20] loss: 1.071\n",
      "[5,    30] loss: 1.032\n",
      "[5,    40] loss: 0.988\n",
      "[5,    50] loss: 1.036\n",
      "[5,    60] loss: 1.001\n",
      "[5,    70] loss: 1.023\n",
      "[5,    80] loss: 0.948\n",
      "[5,    90] loss: 1.004\n",
      "[5,   100] loss: 0.969\n",
      "[5,   110] loss: 0.914\n",
      "[5,   120] loss: 1.121\n",
      "[5,   130] loss: 0.950\n",
      "[5,   140] loss: 0.986\n",
      "[5,   150] loss: 0.985\n",
      "[5,   160] loss: 1.077\n",
      "[5,   170] loss: 1.003\n",
      "[5,   180] loss: 1.003\n",
      "[5,   190] loss: 0.838\n",
      "[5,   200] loss: 0.889\n",
      "[5,   210] loss: 0.942\n",
      "[5,   220] loss: 0.962\n",
      "[5,   230] loss: 0.932\n",
      "[5,   240] loss: 0.992\n",
      "[5,   250] loss: 0.857\n",
      "[5,   260] loss: 1.008\n",
      "[5,   270] loss: 0.962\n",
      "[5,   280] loss: 1.034\n",
      "[5,   290] loss: 0.874\n",
      "[5,   300] loss: 0.901\n",
      "[5,   310] loss: 1.015\n",
      "[5,   320] loss: 0.825\n",
      "[5,   330] loss: 0.916\n",
      "[5,   340] loss: 0.963\n",
      "[5,   350] loss: 1.081\n",
      "[5,   360] loss: 0.898\n",
      "[5,   370] loss: 1.008\n",
      "[5,   380] loss: 0.964\n",
      "[5,   390] loss: 1.121\n",
      "[5,   400] loss: 1.082\n",
      "[5,   410] loss: 1.059\n",
      "[5,   420] loss: 1.000\n",
      "[5,   430] loss: 1.053\n",
      "[5,   440] loss: 0.905\n",
      "[5,   450] loss: 0.897\n",
      "[5,   460] loss: 1.155\n",
      "[5,   470] loss: 1.099\n",
      "[5,   480] loss: 0.935\n",
      "[5,   490] loss: 1.037\n",
      "[5,   500] loss: 0.931\n",
      "[5,   510] loss: 1.018\n",
      "[5,   520] loss: 1.047\n",
      "[5,   530] loss: 1.025\n",
      "[5,   540] loss: 1.015\n",
      "[5,   550] loss: 1.000\n",
      "[5,   560] loss: 1.069\n",
      "[5,   570] loss: 0.942\n",
      "[5,   580] loss: 1.158\n",
      "[5,   590] loss: 1.053\n",
      "[5,   600] loss: 1.081\n",
      "[5,   610] loss: 1.003\n",
      "[5,   620] loss: 0.855\n",
      "[5,   630] loss: 1.043\n",
      "[5,   640] loss: 1.089\n",
      "[5,   650] loss: 1.004\n",
      "[5,   660] loss: 1.095\n",
      "[5,   670] loss: 0.907\n",
      "[5,   680] loss: 0.820\n",
      "[5,   690] loss: 1.045\n",
      "[5,   700] loss: 0.959\n",
      "[5,   710] loss: 1.020\n",
      "[5,   720] loss: 1.071\n",
      "[5,   730] loss: 1.037\n",
      "[5,   740] loss: 1.089\n",
      "[5,   750] loss: 1.024\n",
      "[5,   760] loss: 1.027\n",
      "[5,   770] loss: 0.952\n",
      "[5,   780] loss: 1.142\n",
      "[5,   790] loss: 1.006\n",
      "[5,   800] loss: 0.943\n",
      "[5,   810] loss: 1.023\n",
      "[5,   820] loss: 1.093\n",
      "[5,   830] loss: 1.225\n",
      "[5,   840] loss: 1.126\n",
      "[5,   850] loss: 0.985\n",
      "[5,   860] loss: 1.066\n",
      "[5,   870] loss: 1.216\n",
      "[5,   880] loss: 1.251\n",
      "[5,   890] loss: 1.201\n",
      "[5,   900] loss: 0.955\n",
      "[5,   910] loss: 1.092\n",
      "[5,   920] loss: 1.010\n",
      "[5,   930] loss: 0.934\n",
      "[5,   940] loss: 1.099\n",
      "[5,   950] loss: 0.947\n",
      "[5,   960] loss: 1.068\n",
      "[5,   970] loss: 0.997\n",
      "[5,   980] loss: 1.067\n",
      "[5,   990] loss: 1.006\n",
      "[5,  1000] loss: 1.041\n",
      "[5,  1010] loss: 0.962\n",
      "[5,  1020] loss: 0.956\n",
      "[5,  1030] loss: 1.149\n",
      "[5,  1040] loss: 1.094\n",
      "[5,  1050] loss: 1.199\n",
      "[5,  1060] loss: 1.080\n",
      "[5,  1070] loss: 1.141\n",
      "[5,  1080] loss: 1.120\n",
      "[5,  1090] loss: 1.058\n",
      "[5,  1100] loss: 1.020\n",
      "[5,  1110] loss: 1.171\n",
      "[5,  1120] loss: 0.982\n",
      "[5,  1130] loss: 1.236\n",
      "[5,  1140] loss: 1.009\n",
      "[5,  1150] loss: 1.007\n",
      "[5,  1160] loss: 1.075\n",
      "[5,  1170] loss: 0.933\n",
      "[5,  1180] loss: 1.144\n",
      "[5,  1190] loss: 1.034\n",
      "[5,  1200] loss: 1.019\n",
      "Learning rate: 0.010000\n",
      "[6,    10] loss: 0.993\n",
      "[6,    20] loss: 0.956\n",
      "[6,    30] loss: 0.796\n",
      "[6,    40] loss: 0.938\n",
      "[6,    50] loss: 0.927\n",
      "[6,    60] loss: 0.840\n",
      "[6,    70] loss: 0.758\n",
      "[6,    80] loss: 0.846\n",
      "[6,    90] loss: 0.856\n",
      "[6,   100] loss: 0.837\n",
      "[6,   110] loss: 0.911\n",
      "[6,   120] loss: 0.838\n",
      "[6,   130] loss: 0.870\n",
      "[6,   140] loss: 0.703\n",
      "[6,   150] loss: 0.883\n",
      "[6,   160] loss: 0.857\n",
      "[6,   170] loss: 0.893\n",
      "[6,   180] loss: 0.769\n",
      "[6,   190] loss: 0.879\n",
      "[6,   200] loss: 0.803\n",
      "[6,   210] loss: 0.917\n",
      "[6,   220] loss: 0.925\n",
      "[6,   230] loss: 0.838\n",
      "[6,   240] loss: 0.926\n",
      "[6,   250] loss: 0.843\n",
      "[6,   260] loss: 0.757\n",
      "[6,   270] loss: 1.001\n",
      "[6,   280] loss: 0.900\n",
      "[6,   290] loss: 0.865\n",
      "[6,   300] loss: 0.828\n",
      "[6,   310] loss: 0.848\n",
      "[6,   320] loss: 0.890\n",
      "[6,   330] loss: 0.854\n",
      "[6,   340] loss: 0.893\n",
      "[6,   350] loss: 0.841\n",
      "[6,   360] loss: 1.039\n",
      "[6,   370] loss: 0.928\n",
      "[6,   380] loss: 0.994\n",
      "[6,   390] loss: 0.774\n",
      "[6,   400] loss: 1.029\n",
      "[6,   410] loss: 0.953\n",
      "[6,   420] loss: 0.934\n",
      "[6,   430] loss: 0.862\n",
      "[6,   440] loss: 1.060\n",
      "[6,   450] loss: 0.964\n",
      "[6,   460] loss: 0.978\n",
      "[6,   470] loss: 1.136\n",
      "[6,   480] loss: 0.885\n",
      "[6,   490] loss: 1.020\n",
      "[6,   500] loss: 0.959\n",
      "[6,   510] loss: 0.979\n",
      "[6,   520] loss: 0.867\n",
      "[6,   530] loss: 1.081\n",
      "[6,   540] loss: 0.999\n",
      "[6,   550] loss: 0.973\n",
      "[6,   560] loss: 0.922\n",
      "[6,   570] loss: 0.949\n",
      "[6,   580] loss: 0.958\n",
      "[6,   590] loss: 0.955\n",
      "[6,   600] loss: 0.926\n",
      "[6,   610] loss: 0.918\n",
      "[6,   620] loss: 0.988\n",
      "[6,   630] loss: 1.023\n",
      "[6,   640] loss: 0.973\n",
      "[6,   650] loss: 0.870\n",
      "[6,   660] loss: 0.985\n",
      "[6,   670] loss: 1.007\n",
      "[6,   680] loss: 1.053\n",
      "[6,   690] loss: 0.877\n",
      "[6,   700] loss: 0.918\n",
      "[6,   710] loss: 0.959\n",
      "[6,   720] loss: 1.031\n",
      "[6,   730] loss: 0.940\n",
      "[6,   740] loss: 1.059\n",
      "[6,   750] loss: 0.949\n",
      "[6,   760] loss: 0.911\n",
      "[6,   770] loss: 0.929\n",
      "[6,   780] loss: 0.916\n",
      "[6,   790] loss: 0.930\n",
      "[6,   800] loss: 0.977\n",
      "[6,   810] loss: 0.893\n",
      "[6,   820] loss: 0.956\n",
      "[6,   830] loss: 1.149\n",
      "[6,   840] loss: 0.924\n",
      "[6,   850] loss: 0.977\n",
      "[6,   860] loss: 0.997\n",
      "[6,   870] loss: 0.841\n",
      "[6,   880] loss: 0.963\n",
      "[6,   890] loss: 1.040\n",
      "[6,   900] loss: 1.056\n",
      "[6,   910] loss: 1.042\n",
      "[6,   920] loss: 0.923\n",
      "[6,   930] loss: 0.940\n",
      "[6,   940] loss: 0.934\n",
      "[6,   950] loss: 1.089\n",
      "[6,   960] loss: 1.008\n",
      "[6,   970] loss: 1.084\n",
      "[6,   980] loss: 1.012\n",
      "[6,   990] loss: 0.915\n",
      "[6,  1000] loss: 0.970\n",
      "[6,  1010] loss: 1.009\n",
      "[6,  1020] loss: 0.964\n",
      "[6,  1030] loss: 0.996\n",
      "[6,  1040] loss: 0.920\n",
      "[6,  1050] loss: 0.964\n",
      "[6,  1060] loss: 0.979\n",
      "[6,  1070] loss: 1.045\n",
      "[6,  1080] loss: 0.995\n",
      "[6,  1090] loss: 0.963\n",
      "[6,  1100] loss: 0.950\n",
      "[6,  1110] loss: 1.069\n",
      "[6,  1120] loss: 1.005\n",
      "[6,  1130] loss: 0.932\n",
      "[6,  1140] loss: 0.981\n",
      "[6,  1150] loss: 0.973\n",
      "[6,  1160] loss: 1.022\n",
      "[6,  1170] loss: 1.239\n",
      "[6,  1180] loss: 0.986\n",
      "[6,  1190] loss: 1.014\n",
      "[6,  1200] loss: 0.942\n",
      "Learning rate: 0.010000\n",
      "[7,    10] loss: 1.077\n",
      "[7,    20] loss: 1.099\n",
      "[7,    30] loss: 1.100\n",
      "[7,    40] loss: 1.008\n",
      "[7,    50] loss: 0.849\n",
      "[7,    60] loss: 1.005\n",
      "[7,    70] loss: 0.868\n",
      "[7,    80] loss: 0.876\n",
      "[7,    90] loss: 0.944\n",
      "[7,   100] loss: 0.755\n",
      "[7,   110] loss: 0.937\n",
      "[7,   120] loss: 0.899\n",
      "[7,   130] loss: 0.785\n",
      "[7,   140] loss: 0.785\n",
      "[7,   150] loss: 0.820\n",
      "[7,   160] loss: 0.770\n",
      "[7,   170] loss: 0.791\n",
      "[7,   180] loss: 0.847\n",
      "[7,   190] loss: 0.859\n",
      "[7,   200] loss: 0.874\n",
      "[7,   210] loss: 0.717\n",
      "[7,   220] loss: 0.999\n",
      "[7,   230] loss: 0.753\n",
      "[7,   240] loss: 0.698\n",
      "[7,   250] loss: 0.799\n",
      "[7,   260] loss: 0.844\n",
      "[7,   270] loss: 0.869\n",
      "[7,   280] loss: 0.814\n",
      "[7,   290] loss: 0.838\n",
      "[7,   300] loss: 0.823\n",
      "[7,   310] loss: 0.872\n",
      "[7,   320] loss: 0.849\n",
      "[7,   330] loss: 0.810\n",
      "[7,   340] loss: 0.846\n",
      "[7,   350] loss: 0.862\n",
      "[7,   360] loss: 0.844\n",
      "[7,   370] loss: 1.007\n",
      "[7,   380] loss: 0.813\n",
      "[7,   390] loss: 0.859\n",
      "[7,   400] loss: 0.764\n",
      "[7,   410] loss: 0.851\n",
      "[7,   420] loss: 0.890\n",
      "[7,   430] loss: 0.741\n",
      "[7,   440] loss: 0.911\n",
      "[7,   450] loss: 0.779\n",
      "[7,   460] loss: 0.978\n",
      "[7,   470] loss: 0.888\n",
      "[7,   480] loss: 0.852\n",
      "[7,   490] loss: 0.895\n",
      "[7,   500] loss: 0.868\n",
      "[7,   510] loss: 0.955\n",
      "[7,   520] loss: 0.903\n",
      "[7,   530] loss: 0.711\n",
      "[7,   540] loss: 0.777\n",
      "[7,   550] loss: 0.830\n",
      "[7,   560] loss: 1.011\n",
      "[7,   570] loss: 0.810\n",
      "[7,   580] loss: 0.922\n",
      "[7,   590] loss: 1.014\n",
      "[7,   600] loss: 0.933\n",
      "[7,   610] loss: 0.843\n",
      "[7,   620] loss: 0.864\n",
      "[7,   630] loss: 0.998\n",
      "[7,   640] loss: 0.855\n",
      "[7,   650] loss: 0.880\n",
      "[7,   660] loss: 1.004\n",
      "[7,   670] loss: 1.009\n",
      "[7,   680] loss: 0.923\n",
      "[7,   690] loss: 0.884\n",
      "[7,   700] loss: 0.794\n",
      "[7,   710] loss: 0.793\n",
      "[7,   720] loss: 0.887\n",
      "[7,   730] loss: 0.883\n",
      "[7,   740] loss: 0.901\n",
      "[7,   750] loss: 0.955\n",
      "[7,   760] loss: 0.982\n",
      "[7,   770] loss: 0.957\n",
      "[7,   780] loss: 0.838\n",
      "[7,   790] loss: 0.885\n",
      "[7,   800] loss: 0.919\n",
      "[7,   810] loss: 0.866\n",
      "[7,   820] loss: 0.792\n",
      "[7,   830] loss: 0.857\n",
      "[7,   840] loss: 0.882\n",
      "[7,   850] loss: 0.887\n",
      "[7,   860] loss: 0.904\n",
      "[7,   870] loss: 0.872\n",
      "[7,   880] loss: 0.878\n",
      "[7,   890] loss: 0.824\n",
      "[7,   900] loss: 0.915\n",
      "[7,   910] loss: 0.868\n",
      "[7,   920] loss: 0.828\n",
      "[7,   930] loss: 0.860\n",
      "[7,   940] loss: 0.904\n",
      "[7,   950] loss: 0.902\n",
      "[7,   960] loss: 0.935\n",
      "[7,   970] loss: 0.938\n",
      "[7,   980] loss: 0.921\n",
      "[7,   990] loss: 0.925\n",
      "[7,  1000] loss: 0.988\n",
      "[7,  1010] loss: 0.942\n",
      "[7,  1020] loss: 0.945\n",
      "[7,  1030] loss: 0.986\n",
      "[7,  1040] loss: 0.878\n",
      "[7,  1050] loss: 0.922\n",
      "[7,  1060] loss: 0.962\n",
      "[7,  1070] loss: 0.896\n",
      "[7,  1080] loss: 0.898\n",
      "[7,  1090] loss: 0.812\n",
      "[7,  1100] loss: 0.867\n",
      "[7,  1110] loss: 0.920\n",
      "[7,  1120] loss: 0.904\n",
      "[7,  1130] loss: 0.926\n",
      "[7,  1140] loss: 1.046\n",
      "[7,  1150] loss: 0.926\n",
      "[7,  1160] loss: 0.849\n",
      "[7,  1170] loss: 0.907\n",
      "[7,  1180] loss: 0.858\n",
      "[7,  1190] loss: 0.785\n",
      "[7,  1200] loss: 0.931\n",
      "Learning rate: 0.010000\n",
      "[8,    10] loss: 0.980\n",
      "[8,    20] loss: 0.906\n",
      "[8,    30] loss: 0.796\n",
      "[8,    40] loss: 0.836\n",
      "[8,    50] loss: 0.820\n",
      "[8,    60] loss: 0.682\n",
      "[8,    70] loss: 0.803\n",
      "[8,    80] loss: 0.719\n",
      "[8,    90] loss: 0.709\n",
      "[8,   100] loss: 0.799\n",
      "[8,   110] loss: 0.746\n",
      "[8,   120] loss: 0.683\n",
      "[8,   130] loss: 0.705\n",
      "[8,   140] loss: 0.560\n",
      "[8,   150] loss: 0.757\n",
      "[8,   160] loss: 0.729\n",
      "[8,   170] loss: 0.806\n",
      "[8,   180] loss: 0.781\n",
      "[8,   190] loss: 0.689\n",
      "[8,   200] loss: 0.903\n",
      "[8,   210] loss: 0.744\n",
      "[8,   220] loss: 0.757\n",
      "[8,   230] loss: 0.819\n",
      "[8,   240] loss: 0.893\n",
      "[8,   250] loss: 0.720\n",
      "[8,   260] loss: 0.676\n",
      "[8,   270] loss: 0.648\n",
      "[8,   280] loss: 0.816\n",
      "[8,   290] loss: 0.792\n",
      "[8,   300] loss: 0.871\n",
      "[8,   310] loss: 0.812\n",
      "[8,   320] loss: 0.866\n",
      "[8,   330] loss: 0.790\n",
      "[8,   340] loss: 0.860\n",
      "[8,   350] loss: 0.818\n",
      "[8,   360] loss: 0.986\n",
      "[8,   370] loss: 0.820\n",
      "[8,   380] loss: 0.784\n",
      "[8,   390] loss: 0.835\n",
      "[8,   400] loss: 0.840\n",
      "[8,   410] loss: 0.799\n",
      "[8,   420] loss: 0.912\n",
      "[8,   430] loss: 0.964\n",
      "[8,   440] loss: 0.907\n",
      "[8,   450] loss: 0.792\n",
      "[8,   460] loss: 0.764\n",
      "[8,   470] loss: 0.829\n",
      "[8,   480] loss: 0.791\n",
      "[8,   490] loss: 0.718\n",
      "[8,   500] loss: 1.043\n",
      "[8,   510] loss: 0.755\n",
      "[8,   520] loss: 0.778\n",
      "[8,   530] loss: 0.820\n",
      "[8,   540] loss: 0.856\n",
      "[8,   550] loss: 0.843\n",
      "[8,   560] loss: 0.842\n",
      "[8,   570] loss: 0.925\n",
      "[8,   580] loss: 0.773\n",
      "[8,   590] loss: 0.955\n",
      "[8,   600] loss: 0.798\n",
      "[8,   610] loss: 0.810\n",
      "[8,   620] loss: 0.756\n",
      "[8,   630] loss: 0.879\n",
      "[8,   640] loss: 0.823\n",
      "[8,   650] loss: 0.861\n",
      "[8,   660] loss: 0.878\n",
      "[8,   670] loss: 0.853\n",
      "[8,   680] loss: 0.735\n",
      "[8,   690] loss: 0.841\n",
      "[8,   700] loss: 0.957\n",
      "[8,   710] loss: 0.841\n",
      "[8,   720] loss: 0.829\n",
      "[8,   730] loss: 0.900\n",
      "[8,   740] loss: 0.854\n",
      "[8,   750] loss: 0.845\n",
      "[8,   760] loss: 0.888\n",
      "[8,   770] loss: 0.863\n",
      "[8,   780] loss: 0.788\n",
      "[8,   790] loss: 0.845\n",
      "[8,   800] loss: 0.782\n",
      "[8,   810] loss: 0.958\n",
      "[8,   820] loss: 0.910\n",
      "[8,   830] loss: 0.846\n",
      "[8,   840] loss: 0.748\n",
      "[8,   850] loss: 0.712\n",
      "[8,   860] loss: 0.881\n",
      "[8,   870] loss: 0.921\n",
      "[8,   880] loss: 0.733\n",
      "[8,   890] loss: 0.875\n",
      "[8,   900] loss: 0.859\n",
      "[8,   910] loss: 0.869\n",
      "[8,   920] loss: 0.684\n",
      "[8,   930] loss: 0.777\n",
      "[8,   940] loss: 0.917\n",
      "[8,   950] loss: 0.961\n",
      "[8,   960] loss: 0.817\n",
      "[8,   970] loss: 0.914\n",
      "[8,   980] loss: 0.806\n",
      "[8,   990] loss: 0.876\n",
      "[8,  1000] loss: 0.915\n",
      "[8,  1010] loss: 0.804\n",
      "[8,  1020] loss: 0.818\n",
      "[8,  1030] loss: 0.838\n",
      "[8,  1040] loss: 0.892\n",
      "[8,  1050] loss: 0.839\n",
      "[8,  1060] loss: 0.854\n",
      "[8,  1070] loss: 0.870\n",
      "[8,  1080] loss: 0.793\n",
      "[8,  1090] loss: 0.842\n",
      "[8,  1100] loss: 0.897\n",
      "[8,  1110] loss: 0.914\n",
      "[8,  1120] loss: 0.882\n",
      "[8,  1130] loss: 0.813\n",
      "[8,  1140] loss: 0.893\n",
      "[8,  1150] loss: 0.947\n",
      "[8,  1160] loss: 0.791\n",
      "[8,  1170] loss: 0.821\n",
      "[8,  1180] loss: 0.870\n",
      "[8,  1190] loss: 0.894\n",
      "[8,  1200] loss: 0.810\n",
      "Learning rate: 0.010000\n",
      "[9,    10] loss: 0.841\n",
      "[9,    20] loss: 0.950\n",
      "[9,    30] loss: 0.846\n",
      "[9,    40] loss: 0.811\n",
      "[9,    50] loss: 0.771\n",
      "[9,    60] loss: 0.703\n",
      "[9,    70] loss: 0.698\n",
      "[9,    80] loss: 0.660\n",
      "[9,    90] loss: 0.789\n",
      "[9,   100] loss: 0.786\n",
      "[9,   110] loss: 0.783\n",
      "[9,   120] loss: 0.741\n",
      "[9,   130] loss: 0.657\n",
      "[9,   140] loss: 0.681\n",
      "[9,   150] loss: 0.728\n",
      "[9,   160] loss: 0.646\n",
      "[9,   170] loss: 0.831\n",
      "[9,   180] loss: 0.834\n",
      "[9,   190] loss: 0.756\n",
      "[9,   200] loss: 0.619\n",
      "[9,   210] loss: 0.701\n",
      "[9,   220] loss: 0.813\n",
      "[9,   230] loss: 0.840\n",
      "[9,   240] loss: 0.744\n",
      "[9,   250] loss: 0.712\n",
      "[9,   260] loss: 0.731\n",
      "[9,   270] loss: 0.849\n",
      "[9,   280] loss: 0.800\n",
      "[9,   290] loss: 0.743\n",
      "[9,   300] loss: 0.761\n",
      "[9,   310] loss: 0.770\n",
      "[9,   320] loss: 0.660\n",
      "[9,   330] loss: 0.785\n",
      "[9,   340] loss: 0.705\n",
      "[9,   350] loss: 0.722\n",
      "[9,   360] loss: 0.716\n",
      "[9,   370] loss: 0.742\n",
      "[9,   380] loss: 0.710\n",
      "[9,   390] loss: 0.735\n",
      "[9,   400] loss: 0.640\n",
      "[9,   410] loss: 0.716\n",
      "[9,   420] loss: 0.731\n",
      "[9,   430] loss: 0.828\n",
      "[9,   440] loss: 0.850\n",
      "[9,   450] loss: 0.822\n",
      "[9,   460] loss: 0.811\n",
      "[9,   470] loss: 0.901\n",
      "[9,   480] loss: 0.951\n",
      "[9,   490] loss: 0.949\n",
      "[9,   500] loss: 0.851\n",
      "[9,   510] loss: 0.842\n",
      "[9,   520] loss: 0.832\n",
      "[9,   530] loss: 0.779\n",
      "[9,   540] loss: 0.730\n",
      "[9,   550] loss: 1.004\n",
      "[9,   560] loss: 0.890\n",
      "[9,   570] loss: 0.864\n",
      "[9,   580] loss: 0.935\n",
      "[9,   590] loss: 0.683\n",
      "[9,   600] loss: 0.806\n",
      "[9,   610] loss: 0.790\n",
      "[9,   620] loss: 0.856\n",
      "[9,   630] loss: 0.716\n",
      "[9,   640] loss: 0.800\n",
      "[9,   650] loss: 0.794\n",
      "[9,   660] loss: 0.756\n",
      "[9,   670] loss: 0.841\n",
      "[9,   680] loss: 0.848\n",
      "[9,   690] loss: 0.778\n",
      "[9,   700] loss: 0.722\n",
      "[9,   710] loss: 0.839\n",
      "[9,   720] loss: 0.861\n",
      "[9,   730] loss: 0.667\n",
      "[9,   740] loss: 0.688\n",
      "[9,   750] loss: 0.782\n",
      "[9,   760] loss: 0.837\n",
      "[9,   770] loss: 0.709\n",
      "[9,   780] loss: 0.810\n",
      "[9,   790] loss: 0.705\n",
      "[9,   800] loss: 0.789\n",
      "[9,   810] loss: 0.789\n",
      "[9,   820] loss: 0.964\n",
      "[9,   830] loss: 0.713\n",
      "[9,   840] loss: 0.859\n",
      "[9,   850] loss: 0.881\n",
      "[9,   860] loss: 0.816\n",
      "[9,   870] loss: 0.875\n",
      "[9,   880] loss: 0.908\n",
      "[9,   890] loss: 0.813\n",
      "[9,   900] loss: 0.838\n",
      "[9,   910] loss: 0.893\n",
      "[9,   920] loss: 0.775\n",
      "[9,   930] loss: 0.796\n",
      "[9,   940] loss: 0.639\n",
      "[9,   950] loss: 0.816\n",
      "[9,   960] loss: 0.816\n",
      "[9,   970] loss: 0.751\n",
      "[9,   980] loss: 0.840\n",
      "[9,   990] loss: 0.809\n",
      "[9,  1000] loss: 0.777\n",
      "[9,  1010] loss: 0.704\n",
      "[9,  1020] loss: 0.793\n",
      "[9,  1030] loss: 0.732\n",
      "[9,  1040] loss: 0.932\n",
      "[9,  1050] loss: 0.837\n",
      "[9,  1060] loss: 0.774\n",
      "[9,  1070] loss: 0.909\n",
      "[9,  1080] loss: 0.700\n",
      "[9,  1090] loss: 0.777\n",
      "[9,  1100] loss: 0.943\n",
      "[9,  1110] loss: 0.735\n",
      "[9,  1120] loss: 0.850\n",
      "[9,  1130] loss: 0.796\n",
      "[9,  1140] loss: 0.855\n",
      "[9,  1150] loss: 0.801\n",
      "[9,  1160] loss: 0.764\n",
      "[9,  1170] loss: 0.778\n",
      "[9,  1180] loss: 0.840\n",
      "[9,  1190] loss: 0.689\n",
      "[9,  1200] loss: 0.836\n",
      "Learning rate: 0.010000\n",
      "[10,    10] loss: 0.864\n",
      "[10,    20] loss: 0.796\n",
      "[10,    30] loss: 0.869\n",
      "[10,    40] loss: 0.749\n",
      "[10,    50] loss: 0.720\n",
      "[10,    60] loss: 0.678\n",
      "[10,    70] loss: 0.671\n",
      "[10,    80] loss: 0.750\n",
      "[10,    90] loss: 0.687\n",
      "[10,   100] loss: 0.781\n",
      "[10,   110] loss: 0.741\n",
      "[10,   120] loss: 0.666\n",
      "[10,   130] loss: 0.740\n",
      "[10,   140] loss: 0.678\n",
      "[10,   150] loss: 0.647\n",
      "[10,   160] loss: 0.709\n",
      "[10,   170] loss: 0.786\n",
      "[10,   180] loss: 0.682\n",
      "[10,   190] loss: 0.719\n",
      "[10,   200] loss: 0.677\n",
      "[10,   210] loss: 0.802\n",
      "[10,   220] loss: 0.700\n",
      "[10,   230] loss: 0.731\n",
      "[10,   240] loss: 0.743\n",
      "[10,   250] loss: 0.845\n",
      "[10,   260] loss: 0.692\n",
      "[10,   270] loss: 0.680\n",
      "[10,   280] loss: 0.834\n",
      "[10,   290] loss: 0.808\n",
      "[10,   300] loss: 0.679\n",
      "[10,   310] loss: 0.796\n",
      "[10,   320] loss: 0.694\n",
      "[10,   330] loss: 0.688\n",
      "[10,   340] loss: 0.720\n",
      "[10,   350] loss: 0.712\n",
      "[10,   360] loss: 0.662\n",
      "[10,   370] loss: 0.705\n",
      "[10,   380] loss: 0.688\n",
      "[10,   390] loss: 0.649\n",
      "[10,   400] loss: 0.685\n",
      "[10,   410] loss: 0.626\n",
      "[10,   420] loss: 0.715\n",
      "[10,   430] loss: 0.801\n",
      "[10,   440] loss: 0.795\n",
      "[10,   450] loss: 0.782\n",
      "[10,   460] loss: 0.757\n",
      "[10,   470] loss: 0.788\n",
      "[10,   480] loss: 0.609\n",
      "[10,   490] loss: 0.704\n",
      "[10,   500] loss: 0.729\n",
      "[10,   510] loss: 0.786\n",
      "[10,   520] loss: 0.721\n",
      "[10,   530] loss: 0.734\n",
      "[10,   540] loss: 0.658\n",
      "[10,   550] loss: 0.754\n",
      "[10,   560] loss: 0.644\n",
      "[10,   570] loss: 0.655\n",
      "[10,   580] loss: 0.614\n",
      "[10,   590] loss: 0.764\n",
      "[10,   600] loss: 0.812\n",
      "[10,   610] loss: 0.663\n",
      "[10,   620] loss: 0.696\n",
      "[10,   630] loss: 0.665\n",
      "[10,   640] loss: 0.826\n",
      "[10,   650] loss: 0.772\n",
      "[10,   660] loss: 0.791\n",
      "[10,   670] loss: 0.776\n",
      "[10,   680] loss: 0.738\n",
      "[10,   690] loss: 0.867\n",
      "[10,   700] loss: 0.835\n",
      "[10,   710] loss: 0.848\n",
      "[10,   720] loss: 0.764\n",
      "[10,   730] loss: 0.808\n",
      "[10,   740] loss: 0.781\n",
      "[10,   750] loss: 0.959\n",
      "[10,   760] loss: 0.852\n",
      "[10,   770] loss: 0.788\n",
      "[10,   780] loss: 0.812\n",
      "[10,   790] loss: 0.759\n",
      "[10,   800] loss: 0.665\n",
      "[10,   810] loss: 0.738\n",
      "[10,   820] loss: 0.644\n",
      "[10,   830] loss: 0.654\n",
      "[10,   840] loss: 0.764\n",
      "[10,   850] loss: 0.783\n",
      "[10,   860] loss: 0.764\n",
      "[10,   870] loss: 0.846\n",
      "[10,   880] loss: 0.851\n",
      "[10,   890] loss: 0.763\n",
      "[10,   900] loss: 0.833\n",
      "[10,   910] loss: 0.750\n",
      "[10,   920] loss: 0.753\n",
      "[10,   930] loss: 0.846\n",
      "[10,   940] loss: 0.887\n",
      "[10,   950] loss: 0.809\n",
      "[10,   960] loss: 0.757\n",
      "[10,   970] loss: 0.805\n",
      "[10,   980] loss: 0.845\n",
      "[10,   990] loss: 0.828\n",
      "[10,  1000] loss: 0.781\n",
      "[10,  1010] loss: 0.655\n",
      "[10,  1020] loss: 0.706\n",
      "[10,  1030] loss: 0.815\n",
      "[10,  1040] loss: 0.873\n",
      "[10,  1050] loss: 0.725\n",
      "[10,  1060] loss: 0.888\n",
      "[10,  1070] loss: 0.770\n",
      "[10,  1080] loss: 0.746\n",
      "[10,  1090] loss: 0.838\n",
      "[10,  1100] loss: 0.756\n",
      "[10,  1110] loss: 0.766\n",
      "[10,  1120] loss: 0.730\n",
      "[10,  1130] loss: 0.852\n",
      "[10,  1140] loss: 0.733\n",
      "[10,  1150] loss: 0.819\n",
      "[10,  1160] loss: 0.778\n",
      "[10,  1170] loss: 0.791\n",
      "[10,  1180] loss: 0.815\n",
      "[10,  1190] loss: 0.885\n",
      "[10,  1200] loss: 0.890\n",
      "Learning rate: 0.010000\n",
      "[11,    10] loss: 0.827\n",
      "[11,    20] loss: 0.771\n",
      "[11,    30] loss: 0.631\n",
      "[11,    40] loss: 0.656\n",
      "[11,    50] loss: 0.656\n",
      "[11,    60] loss: 0.674\n",
      "[11,    70] loss: 0.672\n",
      "[11,    80] loss: 0.625\n",
      "[11,    90] loss: 0.658\n",
      "[11,   100] loss: 0.642\n",
      "[11,   110] loss: 0.681\n",
      "[11,   120] loss: 0.712\n",
      "[11,   130] loss: 0.682\n",
      "[11,   140] loss: 0.652\n",
      "[11,   150] loss: 0.654\n",
      "[11,   160] loss: 0.606\n",
      "[11,   170] loss: 0.634\n",
      "[11,   180] loss: 0.677\n",
      "[11,   190] loss: 0.744\n",
      "[11,   200] loss: 0.533\n",
      "[11,   210] loss: 0.617\n",
      "[11,   220] loss: 0.643\n",
      "[11,   230] loss: 0.679\n",
      "[11,   240] loss: 0.685\n",
      "[11,   250] loss: 0.656\n",
      "[11,   260] loss: 0.641\n",
      "[11,   270] loss: 0.632\n",
      "[11,   280] loss: 0.671\n",
      "[11,   290] loss: 0.728\n",
      "[11,   300] loss: 0.815\n",
      "[11,   310] loss: 0.649\n",
      "[11,   320] loss: 0.642\n",
      "[11,   330] loss: 0.777\n",
      "[11,   340] loss: 0.657\n",
      "[11,   350] loss: 0.580\n",
      "[11,   360] loss: 0.791\n",
      "[11,   370] loss: 0.670\n",
      "[11,   380] loss: 0.659\n",
      "[11,   390] loss: 0.810\n",
      "[11,   400] loss: 0.717\n",
      "[11,   410] loss: 0.769\n",
      "[11,   420] loss: 0.677\n",
      "[11,   430] loss: 0.806\n",
      "[11,   440] loss: 0.777\n",
      "[11,   450] loss: 0.607\n",
      "[11,   460] loss: 0.758\n",
      "[11,   470] loss: 0.786\n",
      "[11,   480] loss: 0.684\n",
      "[11,   490] loss: 0.697\n",
      "[11,   500] loss: 0.817\n",
      "[11,   510] loss: 0.928\n",
      "[11,   520] loss: 0.614\n",
      "[11,   530] loss: 0.787\n",
      "[11,   540] loss: 0.778\n",
      "[11,   550] loss: 0.667\n",
      "[11,   560] loss: 0.746\n",
      "[11,   570] loss: 0.808\n",
      "[11,   580] loss: 0.732\n",
      "[11,   590] loss: 0.834\n",
      "[11,   600] loss: 0.741\n",
      "[11,   610] loss: 0.759\n",
      "[11,   620] loss: 0.736\n",
      "[11,   630] loss: 0.674\n",
      "[11,   640] loss: 0.617\n",
      "[11,   650] loss: 0.648\n",
      "[11,   660] loss: 0.604\n",
      "[11,   670] loss: 0.721\n",
      "[11,   680] loss: 0.760\n",
      "[11,   690] loss: 0.810\n",
      "[11,   700] loss: 0.748\n",
      "[11,   710] loss: 0.771\n",
      "[11,   720] loss: 0.720\n",
      "[11,   730] loss: 0.737\n",
      "[11,   740] loss: 0.767\n",
      "[11,   750] loss: 0.764\n",
      "[11,   760] loss: 0.784\n",
      "[11,   770] loss: 0.876\n",
      "[11,   780] loss: 0.706\n",
      "[11,   790] loss: 0.795\n",
      "[11,   800] loss: 0.662\n",
      "[11,   810] loss: 0.721\n",
      "[11,   820] loss: 0.727\n",
      "[11,   830] loss: 0.736\n",
      "[11,   840] loss: 0.633\n",
      "[11,   850] loss: 0.686\n",
      "[11,   860] loss: 0.665\n",
      "[11,   870] loss: 0.720\n",
      "[11,   880] loss: 0.741\n",
      "[11,   890] loss: 0.679\n",
      "[11,   900] loss: 0.815\n",
      "[11,   910] loss: 0.767\n",
      "[11,   920] loss: 0.699\n",
      "[11,   930] loss: 0.751\n",
      "[11,   940] loss: 0.721\n",
      "[11,   950] loss: 0.775\n",
      "[11,   960] loss: 0.698\n",
      "[11,   970] loss: 0.711\n",
      "[11,   980] loss: 0.710\n",
      "[11,   990] loss: 0.726\n",
      "[11,  1000] loss: 0.702\n",
      "[11,  1010] loss: 0.704\n",
      "[11,  1020] loss: 0.835\n",
      "[11,  1030] loss: 0.615\n",
      "[11,  1040] loss: 0.767\n",
      "[11,  1050] loss: 0.871\n",
      "[11,  1060] loss: 0.842\n",
      "[11,  1070] loss: 0.888\n",
      "[11,  1080] loss: 0.800\n",
      "[11,  1090] loss: 0.811\n",
      "[11,  1100] loss: 0.703\n",
      "[11,  1110] loss: 0.784\n",
      "[11,  1120] loss: 0.776\n",
      "[11,  1130] loss: 0.839\n",
      "[11,  1140] loss: 0.808\n",
      "[11,  1150] loss: 0.759\n",
      "[11,  1160] loss: 0.814\n",
      "[11,  1170] loss: 0.790\n",
      "[11,  1180] loss: 0.786\n",
      "[11,  1190] loss: 0.757\n",
      "[11,  1200] loss: 0.784\n",
      "Learning rate: 0.010000\n",
      "[12,    10] loss: 1.061\n",
      "[12,    20] loss: 0.877\n",
      "[12,    30] loss: 0.766\n",
      "[12,    40] loss: 0.740\n",
      "[12,    50] loss: 0.727\n",
      "[12,    60] loss: 0.712\n",
      "[12,    70] loss: 0.647\n",
      "[12,    80] loss: 0.781\n",
      "[12,    90] loss: 0.638\n",
      "[12,   100] loss: 0.628\n",
      "[12,   110] loss: 0.737\n",
      "[12,   120] loss: 0.669\n",
      "[12,   130] loss: 0.697\n",
      "[12,   140] loss: 0.547\n",
      "[12,   150] loss: 0.714\n",
      "[12,   160] loss: 0.577\n",
      "[12,   170] loss: 0.602\n",
      "[12,   180] loss: 0.758\n",
      "[12,   190] loss: 0.723\n",
      "[12,   200] loss: 0.582\n",
      "[12,   210] loss: 0.704\n",
      "[12,   220] loss: 0.669\n",
      "[12,   230] loss: 0.677\n",
      "[12,   240] loss: 0.783\n",
      "[12,   250] loss: 0.678\n",
      "[12,   260] loss: 0.764\n",
      "[12,   270] loss: 0.720\n",
      "[12,   280] loss: 0.659\n",
      "[12,   290] loss: 0.678\n",
      "[12,   300] loss: 0.648\n",
      "[12,   310] loss: 0.588\n",
      "[12,   320] loss: 0.664\n",
      "[12,   330] loss: 0.593\n",
      "[12,   340] loss: 0.608\n",
      "[12,   350] loss: 0.692\n",
      "[12,   360] loss: 0.692\n",
      "[12,   370] loss: 0.605\n",
      "[12,   380] loss: 0.619\n",
      "[12,   390] loss: 0.638\n",
      "[12,   400] loss: 0.630\n",
      "[12,   410] loss: 0.685\n",
      "[12,   420] loss: 0.611\n",
      "[12,   430] loss: 0.708\n",
      "[12,   440] loss: 0.742\n",
      "[12,   450] loss: 0.727\n",
      "[12,   460] loss: 0.683\n",
      "[12,   470] loss: 0.734\n",
      "[12,   480] loss: 0.667\n",
      "[12,   490] loss: 0.683\n",
      "[12,   500] loss: 0.649\n",
      "[12,   510] loss: 0.703\n",
      "[12,   520] loss: 0.724\n",
      "[12,   530] loss: 0.661\n",
      "[12,   540] loss: 0.791\n",
      "[12,   550] loss: 0.666\n",
      "[12,   560] loss: 0.753\n",
      "[12,   570] loss: 0.699\n",
      "[12,   580] loss: 0.676\n",
      "[12,   590] loss: 0.675\n",
      "[12,   600] loss: 0.710\n",
      "[12,   610] loss: 0.843\n",
      "[12,   620] loss: 0.798\n",
      "[12,   630] loss: 0.806\n",
      "[12,   640] loss: 0.680\n",
      "[12,   650] loss: 0.683\n",
      "[12,   660] loss: 0.713\n",
      "[12,   670] loss: 0.657\n",
      "[12,   680] loss: 0.792\n",
      "[12,   690] loss: 0.922\n",
      "[12,   700] loss: 0.745\n",
      "[12,   710] loss: 0.615\n",
      "[12,   720] loss: 0.669\n",
      "[12,   730] loss: 0.616\n",
      "[12,   740] loss: 0.698\n",
      "[12,   750] loss: 0.617\n",
      "[12,   760] loss: 0.739\n",
      "[12,   770] loss: 0.841\n",
      "[12,   780] loss: 0.749\n",
      "[12,   790] loss: 0.726\n",
      "[12,   800] loss: 0.743\n",
      "[12,   810] loss: 0.729\n",
      "[12,   820] loss: 0.697\n",
      "[12,   830] loss: 0.663\n",
      "[12,   840] loss: 0.744\n",
      "[12,   850] loss: 0.601\n",
      "[12,   860] loss: 0.783\n",
      "[12,   870] loss: 0.685\n",
      "[12,   880] loss: 0.772\n",
      "[12,   890] loss: 0.766\n",
      "[12,   900] loss: 0.831\n",
      "[12,   910] loss: 0.824\n",
      "[12,   920] loss: 0.770\n",
      "[12,   930] loss: 0.791\n",
      "[12,   940] loss: 0.707\n",
      "[12,   950] loss: 0.694\n",
      "[12,   960] loss: 0.591\n",
      "[12,   970] loss: 0.624\n",
      "[12,   980] loss: 0.740\n",
      "[12,   990] loss: 0.782\n",
      "[12,  1000] loss: 0.679\n",
      "[12,  1010] loss: 0.788\n",
      "[12,  1020] loss: 0.715\n",
      "[12,  1030] loss: 0.790\n",
      "[12,  1040] loss: 0.820\n",
      "[12,  1050] loss: 0.730\n",
      "[12,  1060] loss: 0.804\n",
      "[12,  1070] loss: 0.749\n",
      "[12,  1080] loss: 0.742\n",
      "[12,  1090] loss: 0.795\n",
      "[12,  1100] loss: 0.853\n",
      "[12,  1110] loss: 0.848\n",
      "[12,  1120] loss: 0.773\n",
      "[12,  1130] loss: 0.897\n",
      "[12,  1140] loss: 0.859\n",
      "[12,  1150] loss: 0.780\n",
      "[12,  1160] loss: 0.767\n",
      "[12,  1170] loss: 0.688\n",
      "[12,  1180] loss: 0.761\n",
      "[12,  1190] loss: 0.748\n",
      "[12,  1200] loss: 0.734\n",
      "Learning rate: 0.010000\n",
      "[13,    10] loss: 0.774\n",
      "[13,    20] loss: 0.885\n",
      "[13,    30] loss: 0.829\n",
      "[13,    40] loss: 0.582\n",
      "[13,    50] loss: 0.754\n",
      "[13,    60] loss: 0.754\n",
      "[13,    70] loss: 0.560\n",
      "[13,    80] loss: 0.585\n",
      "[13,    90] loss: 0.638\n",
      "[13,   100] loss: 0.547\n",
      "[13,   110] loss: 0.560\n",
      "[13,   120] loss: 0.591\n",
      "[13,   130] loss: 0.632\n",
      "[13,   140] loss: 0.533\n",
      "[13,   150] loss: 0.724\n",
      "[13,   160] loss: 0.667\n",
      "[13,   170] loss: 0.614\n",
      "[13,   180] loss: 0.596\n",
      "[13,   190] loss: 0.573\n",
      "[13,   200] loss: 0.637\n",
      "[13,   210] loss: 0.596\n",
      "[13,   220] loss: 0.635\n",
      "[13,   230] loss: 0.741\n",
      "[13,   240] loss: 0.621\n",
      "[13,   250] loss: 0.760\n",
      "[13,   260] loss: 0.672\n",
      "[13,   270] loss: 0.580\n",
      "[13,   280] loss: 0.578\n",
      "[13,   290] loss: 0.681\n",
      "[13,   300] loss: 0.635\n",
      "[13,   310] loss: 0.544\n",
      "[13,   320] loss: 0.710\n",
      "[13,   330] loss: 0.752\n",
      "[13,   340] loss: 0.607\n",
      "[13,   350] loss: 0.724\n",
      "[13,   360] loss: 0.680\n",
      "[13,   370] loss: 0.598\n",
      "[13,   380] loss: 0.627\n",
      "[13,   390] loss: 0.604\n",
      "[13,   400] loss: 0.650\n",
      "[13,   410] loss: 0.663\n",
      "[13,   420] loss: 0.744\n",
      "[13,   430] loss: 0.710\n",
      "[13,   440] loss: 0.606\n",
      "[13,   450] loss: 0.737\n",
      "[13,   460] loss: 0.656\n",
      "[13,   470] loss: 0.620\n",
      "[13,   480] loss: 0.728\n",
      "[13,   490] loss: 0.664\n",
      "[13,   500] loss: 0.575\n",
      "[13,   510] loss: 0.766\n",
      "[13,   520] loss: 0.693\n",
      "[13,   530] loss: 0.619\n",
      "[13,   540] loss: 0.700\n",
      "[13,   550] loss: 0.642\n",
      "[13,   560] loss: 0.654\n",
      "[13,   570] loss: 0.670\n",
      "[13,   580] loss: 0.718\n",
      "[13,   590] loss: 0.685\n",
      "[13,   600] loss: 0.650\n",
      "[13,   610] loss: 0.547\n",
      "[13,   620] loss: 0.647\n",
      "[13,   630] loss: 0.670\n",
      "[13,   640] loss: 0.691\n",
      "[13,   650] loss: 0.621\n",
      "[13,   660] loss: 0.679\n",
      "[13,   670] loss: 0.684\n",
      "[13,   680] loss: 0.681\n",
      "[13,   690] loss: 0.762\n",
      "[13,   700] loss: 0.657\n",
      "[13,   710] loss: 0.520\n",
      "[13,   720] loss: 0.768\n",
      "[13,   730] loss: 0.696\n",
      "[13,   740] loss: 0.761\n",
      "[13,   750] loss: 0.797\n",
      "[13,   760] loss: 0.655\n",
      "[13,   770] loss: 0.719\n",
      "[13,   780] loss: 0.677\n",
      "[13,   790] loss: 0.771\n",
      "[13,   800] loss: 0.715\n",
      "[13,   810] loss: 0.716\n",
      "[13,   820] loss: 0.787\n",
      "[13,   830] loss: 0.682\n",
      "[13,   840] loss: 0.693\n",
      "[13,   850] loss: 0.673\n",
      "[13,   860] loss: 0.784\n",
      "[13,   870] loss: 0.754\n",
      "[13,   880] loss: 0.777\n",
      "[13,   890] loss: 0.799\n",
      "[13,   900] loss: 0.709\n",
      "[13,   910] loss: 0.750\n",
      "[13,   920] loss: 0.801\n",
      "[13,   930] loss: 0.700\n",
      "[13,   940] loss: 0.742\n",
      "[13,   950] loss: 0.835\n",
      "[13,   960] loss: 0.742\n",
      "[13,   970] loss: 0.662\n",
      "[13,   980] loss: 0.829\n",
      "[13,   990] loss: 0.672\n",
      "[13,  1000] loss: 0.707\n",
      "[13,  1010] loss: 0.673\n",
      "[13,  1020] loss: 0.716\n",
      "[13,  1030] loss: 0.922\n",
      "[13,  1040] loss: 0.817\n",
      "[13,  1050] loss: 0.759\n",
      "[13,  1060] loss: 0.686\n",
      "[13,  1070] loss: 0.764\n",
      "[13,  1080] loss: 0.674\n",
      "[13,  1090] loss: 0.681\n",
      "[13,  1100] loss: 0.716\n",
      "[13,  1110] loss: 0.796\n",
      "[13,  1120] loss: 0.652\n",
      "[13,  1130] loss: 0.709\n",
      "[13,  1140] loss: 0.735\n",
      "[13,  1150] loss: 0.792\n",
      "[13,  1160] loss: 0.701\n",
      "[13,  1170] loss: 0.768\n",
      "[13,  1180] loss: 0.754\n",
      "[13,  1190] loss: 0.750\n",
      "[13,  1200] loss: 0.708\n",
      "Learning rate: 0.010000\n",
      "[14,    10] loss: 0.908\n",
      "[14,    20] loss: 0.942\n",
      "[14,    30] loss: 0.860\n",
      "[14,    40] loss: 0.656\n",
      "[14,    50] loss: 0.708\n",
      "[14,    60] loss: 0.698\n",
      "[14,    70] loss: 0.733\n",
      "[14,    80] loss: 0.687\n",
      "[14,    90] loss: 0.668\n",
      "[14,   100] loss: 0.554\n",
      "[14,   110] loss: 0.548\n",
      "[14,   120] loss: 0.603\n",
      "[14,   130] loss: 0.647\n",
      "[14,   140] loss: 0.712\n",
      "[14,   150] loss: 0.699\n",
      "[14,   160] loss: 0.589\n",
      "[14,   170] loss: 0.657\n",
      "[14,   180] loss: 0.659\n",
      "[14,   190] loss: 0.600\n",
      "[14,   200] loss: 0.559\n",
      "[14,   210] loss: 0.632\n",
      "[14,   220] loss: 0.610\n",
      "[14,   230] loss: 0.595\n",
      "[14,   240] loss: 0.668\n",
      "[14,   250] loss: 0.648\n",
      "[14,   260] loss: 0.641\n",
      "[14,   270] loss: 0.616\n",
      "[14,   280] loss: 0.647\n",
      "[14,   290] loss: 0.606\n",
      "[14,   300] loss: 0.589\n",
      "[14,   310] loss: 0.628\n",
      "[14,   320] loss: 0.610\n",
      "[14,   330] loss: 0.793\n",
      "[14,   340] loss: 0.554\n",
      "[14,   350] loss: 0.713\n",
      "[14,   360] loss: 0.793\n",
      "[14,   370] loss: 0.596\n",
      "[14,   380] loss: 0.624\n",
      "[14,   390] loss: 0.650\n",
      "[14,   400] loss: 0.646\n",
      "[14,   410] loss: 0.660\n",
      "[14,   420] loss: 0.647\n",
      "[14,   430] loss: 0.626\n",
      "[14,   440] loss: 0.597\n",
      "[14,   450] loss: 0.669\n",
      "[14,   460] loss: 0.719\n",
      "[14,   470] loss: 0.776\n",
      "[14,   480] loss: 0.752\n",
      "[14,   490] loss: 0.794\n",
      "[14,   500] loss: 0.646\n",
      "[14,   510] loss: 0.692\n",
      "[14,   520] loss: 0.773\n",
      "[14,   530] loss: 0.666\n",
      "[14,   540] loss: 0.701\n",
      "[14,   550] loss: 0.609\n",
      "[14,   560] loss: 0.677\n",
      "[14,   570] loss: 0.624\n",
      "[14,   580] loss: 0.616\n",
      "[14,   590] loss: 0.656\n",
      "[14,   600] loss: 0.574\n",
      "[14,   610] loss: 0.716\n",
      "[14,   620] loss: 0.716\n",
      "[14,   630] loss: 0.629\n",
      "[14,   640] loss: 0.724\n",
      "[14,   650] loss: 0.728\n",
      "[14,   660] loss: 0.684\n",
      "[14,   670] loss: 0.738\n",
      "[14,   680] loss: 0.822\n",
      "[14,   690] loss: 0.607\n",
      "[14,   700] loss: 0.702\n",
      "[14,   710] loss: 0.657\n",
      "[14,   720] loss: 0.716\n",
      "[14,   730] loss: 0.785\n",
      "[14,   740] loss: 0.714\n",
      "[14,   750] loss: 0.633\n",
      "[14,   760] loss: 0.804\n",
      "[14,   770] loss: 0.669\n",
      "[14,   780] loss: 0.770\n",
      "[14,   790] loss: 0.717\n",
      "[14,   800] loss: 0.625\n",
      "[14,   810] loss: 0.818\n",
      "[14,   820] loss: 0.726\n",
      "[14,   830] loss: 0.718\n",
      "[14,   840] loss: 0.778\n",
      "[14,   850] loss: 0.755\n",
      "[14,   860] loss: 0.640\n",
      "[14,   870] loss: 0.746\n",
      "[14,   880] loss: 0.709\n",
      "[14,   890] loss: 0.734\n",
      "[14,   900] loss: 0.777\n",
      "[14,   910] loss: 0.667\n",
      "[14,   920] loss: 0.708\n",
      "[14,   930] loss: 0.658\n",
      "[14,   940] loss: 0.704\n",
      "[14,   950] loss: 0.585\n",
      "[14,   960] loss: 0.702\n",
      "[14,   970] loss: 0.733\n",
      "[14,   980] loss: 0.639\n",
      "[14,   990] loss: 0.842\n",
      "[14,  1000] loss: 0.781\n",
      "[14,  1010] loss: 0.563\n",
      "[14,  1020] loss: 0.778\n",
      "[14,  1030] loss: 0.574\n",
      "[14,  1040] loss: 0.743\n",
      "[14,  1050] loss: 0.741\n",
      "[14,  1060] loss: 0.672\n",
      "[14,  1070] loss: 0.677\n",
      "[14,  1080] loss: 0.688\n",
      "[14,  1090] loss: 0.656\n",
      "[14,  1100] loss: 0.789\n",
      "[14,  1110] loss: 0.788\n",
      "[14,  1120] loss: 0.708\n",
      "[14,  1130] loss: 0.815\n",
      "[14,  1140] loss: 0.758\n",
      "[14,  1150] loss: 0.759\n",
      "[14,  1160] loss: 0.718\n",
      "[14,  1170] loss: 0.671\n",
      "[14,  1180] loss: 0.749\n",
      "[14,  1190] loss: 0.693\n",
      "[14,  1200] loss: 0.765\n",
      "Learning rate: 0.001000\n",
      "[15,    10] loss: 0.610\n",
      "[15,    20] loss: 0.623\n",
      "[15,    30] loss: 0.517\n",
      "[15,    40] loss: 0.508\n",
      "[15,    50] loss: 0.499\n",
      "[15,    60] loss: 0.520\n",
      "[15,    70] loss: 0.510\n",
      "[15,    80] loss: 0.401\n",
      "[15,    90] loss: 0.492\n",
      "[15,   100] loss: 0.404\n",
      "[15,   110] loss: 0.447\n",
      "[15,   120] loss: 0.364\n",
      "[15,   130] loss: 0.377\n",
      "[15,   140] loss: 0.440\n",
      "[15,   150] loss: 0.405\n",
      "[15,   160] loss: 0.453\n",
      "[15,   170] loss: 0.319\n",
      "[15,   180] loss: 0.435\n",
      "[15,   190] loss: 0.367\n",
      "[15,   200] loss: 0.379\n",
      "[15,   210] loss: 0.341\n",
      "[15,   220] loss: 0.411\n",
      "[15,   230] loss: 0.294\n",
      "[15,   240] loss: 0.353\n",
      "[15,   250] loss: 0.346\n",
      "[15,   260] loss: 0.274\n",
      "[15,   270] loss: 0.330\n",
      "[15,   280] loss: 0.369\n",
      "[15,   290] loss: 0.393\n",
      "[15,   300] loss: 0.341\n",
      "[15,   310] loss: 0.319\n",
      "[15,   320] loss: 0.305\n",
      "[15,   330] loss: 0.370\n",
      "[15,   340] loss: 0.361\n",
      "[15,   350] loss: 0.291\n",
      "[15,   360] loss: 0.288\n",
      "[15,   370] loss: 0.330\n",
      "[15,   380] loss: 0.293\n",
      "[15,   390] loss: 0.286\n",
      "[15,   400] loss: 0.258\n",
      "[15,   410] loss: 0.285\n",
      "[15,   420] loss: 0.363\n",
      "[15,   430] loss: 0.321\n",
      "[15,   440] loss: 0.287\n",
      "[15,   450] loss: 0.264\n",
      "[15,   460] loss: 0.330\n",
      "[15,   470] loss: 0.278\n",
      "[15,   480] loss: 0.280\n",
      "[15,   490] loss: 0.346\n",
      "[15,   500] loss: 0.297\n",
      "[15,   510] loss: 0.296\n",
      "[15,   520] loss: 0.298\n",
      "[15,   530] loss: 0.354\n",
      "[15,   540] loss: 0.269\n",
      "[15,   550] loss: 0.269\n",
      "[15,   560] loss: 0.262\n",
      "[15,   570] loss: 0.313\n",
      "[15,   580] loss: 0.273\n",
      "[15,   590] loss: 0.267\n",
      "[15,   600] loss: 0.329\n",
      "[15,   610] loss: 0.273\n",
      "[15,   620] loss: 0.276\n",
      "[15,   630] loss: 0.321\n",
      "[15,   640] loss: 0.271\n",
      "[15,   650] loss: 0.289\n",
      "[15,   660] loss: 0.255\n",
      "[15,   670] loss: 0.349\n",
      "[15,   680] loss: 0.318\n",
      "[15,   690] loss: 0.296\n",
      "[15,   700] loss: 0.311\n",
      "[15,   710] loss: 0.371\n",
      "[15,   720] loss: 0.281\n",
      "[15,   730] loss: 0.225\n",
      "[15,   740] loss: 0.340\n",
      "[15,   750] loss: 0.292\n",
      "[15,   760] loss: 0.345\n",
      "[15,   770] loss: 0.351\n",
      "[15,   780] loss: 0.220\n",
      "[15,   790] loss: 0.340\n",
      "[15,   800] loss: 0.246\n",
      "[15,   810] loss: 0.270\n",
      "[15,   820] loss: 0.315\n",
      "[15,   830] loss: 0.258\n",
      "[15,   840] loss: 0.352\n",
      "[15,   850] loss: 0.260\n",
      "[15,   860] loss: 0.220\n",
      "[15,   870] loss: 0.303\n",
      "[15,   880] loss: 0.238\n",
      "[15,   890] loss: 0.258\n",
      "[15,   900] loss: 0.260\n",
      "[15,   910] loss: 0.278\n",
      "[15,   920] loss: 0.244\n",
      "[15,   930] loss: 0.292\n",
      "[15,   940] loss: 0.288\n",
      "[15,   950] loss: 0.328\n",
      "[15,   960] loss: 0.262\n",
      "[15,   970] loss: 0.231\n",
      "[15,   980] loss: 0.317\n",
      "[15,   990] loss: 0.287\n",
      "[15,  1000] loss: 0.237\n",
      "[15,  1010] loss: 0.223\n",
      "[15,  1020] loss: 0.336\n",
      "[15,  1030] loss: 0.298\n",
      "[15,  1040] loss: 0.270\n",
      "[15,  1050] loss: 0.234\n",
      "[15,  1060] loss: 0.316\n",
      "[15,  1070] loss: 0.257\n",
      "[15,  1080] loss: 0.243\n",
      "[15,  1090] loss: 0.254\n",
      "[15,  1100] loss: 0.323\n",
      "[15,  1110] loss: 0.256\n",
      "[15,  1120] loss: 0.242\n",
      "[15,  1130] loss: 0.213\n",
      "[15,  1140] loss: 0.245\n",
      "[15,  1150] loss: 0.244\n",
      "[15,  1160] loss: 0.362\n",
      "[15,  1170] loss: 0.293\n",
      "[15,  1180] loss: 0.270\n",
      "[15,  1190] loss: 0.206\n",
      "[15,  1200] loss: 0.242\n",
      "Learning rate: 0.001000\n",
      "[16,    10] loss: 0.203\n",
      "[16,    20] loss: 0.230\n",
      "[16,    30] loss: 0.229\n",
      "[16,    40] loss: 0.263\n",
      "[16,    50] loss: 0.212\n",
      "[16,    60] loss: 0.214\n",
      "[16,    70] loss: 0.308\n",
      "[16,    80] loss: 0.209\n",
      "[16,    90] loss: 0.268\n",
      "[16,   100] loss: 0.256\n",
      "[16,   110] loss: 0.211\n",
      "[16,   120] loss: 0.195\n",
      "[16,   130] loss: 0.242\n",
      "[16,   140] loss: 0.296\n",
      "[16,   150] loss: 0.291\n",
      "[16,   160] loss: 0.180\n",
      "[16,   170] loss: 0.210\n",
      "[16,   180] loss: 0.196\n",
      "[16,   190] loss: 0.272\n",
      "[16,   200] loss: 0.200\n",
      "[16,   210] loss: 0.262\n",
      "[16,   220] loss: 0.244\n",
      "[16,   230] loss: 0.218\n",
      "[16,   240] loss: 0.216\n",
      "[16,   250] loss: 0.209\n",
      "[16,   260] loss: 0.282\n",
      "[16,   270] loss: 0.195\n",
      "[16,   280] loss: 0.178\n",
      "[16,   290] loss: 0.232\n",
      "[16,   300] loss: 0.167\n",
      "[16,   310] loss: 0.266\n",
      "[16,   320] loss: 0.173\n",
      "[16,   330] loss: 0.263\n",
      "[16,   340] loss: 0.246\n",
      "[16,   350] loss: 0.227\n",
      "[16,   360] loss: 0.262\n",
      "[16,   370] loss: 0.256\n",
      "[16,   380] loss: 0.214\n",
      "[16,   390] loss: 0.259\n",
      "[16,   400] loss: 0.241\n",
      "[16,   410] loss: 0.233\n",
      "[16,   420] loss: 0.195\n",
      "[16,   430] loss: 0.137\n",
      "[16,   440] loss: 0.215\n",
      "[16,   450] loss: 0.210\n",
      "[16,   460] loss: 0.178\n",
      "[16,   470] loss: 0.182\n",
      "[16,   480] loss: 0.265\n",
      "[16,   490] loss: 0.233\n",
      "[16,   500] loss: 0.263\n",
      "[16,   510] loss: 0.245\n",
      "[16,   520] loss: 0.197\n",
      "[16,   530] loss: 0.285\n",
      "[16,   540] loss: 0.231\n",
      "[16,   550] loss: 0.220\n",
      "[16,   560] loss: 0.180\n",
      "[16,   570] loss: 0.221\n",
      "[16,   580] loss: 0.293\n",
      "[16,   590] loss: 0.186\n",
      "[16,   600] loss: 0.185\n",
      "[16,   610] loss: 0.200\n",
      "[16,   620] loss: 0.228\n",
      "[16,   630] loss: 0.252\n",
      "[16,   640] loss: 0.214\n",
      "[16,   650] loss: 0.262\n",
      "[16,   660] loss: 0.214\n",
      "[16,   670] loss: 0.246\n",
      "[16,   680] loss: 0.262\n",
      "[16,   690] loss: 0.210\n",
      "[16,   700] loss: 0.215\n",
      "[16,   710] loss: 0.242\n",
      "[16,   720] loss: 0.206\n",
      "[16,   730] loss: 0.272\n",
      "[16,   740] loss: 0.284\n",
      "[16,   750] loss: 0.220\n",
      "[16,   760] loss: 0.235\n",
      "[16,   770] loss: 0.176\n",
      "[16,   780] loss: 0.177\n",
      "[16,   790] loss: 0.162\n",
      "[16,   800] loss: 0.270\n",
      "[16,   810] loss: 0.214\n",
      "[16,   820] loss: 0.223\n",
      "[16,   830] loss: 0.214\n",
      "[16,   840] loss: 0.175\n",
      "[16,   850] loss: 0.232\n",
      "[16,   860] loss: 0.245\n",
      "[16,   870] loss: 0.217\n",
      "[16,   880] loss: 0.216\n",
      "[16,   890] loss: 0.225\n",
      "[16,   900] loss: 0.209\n",
      "[16,   910] loss: 0.202\n",
      "[16,   920] loss: 0.209\n",
      "[16,   930] loss: 0.218\n",
      "[16,   940] loss: 0.223\n",
      "[16,   950] loss: 0.189\n",
      "[16,   960] loss: 0.203\n",
      "[16,   970] loss: 0.222\n",
      "[16,   980] loss: 0.310\n",
      "[16,   990] loss: 0.194\n",
      "[16,  1000] loss: 0.218\n",
      "[16,  1010] loss: 0.239\n",
      "[16,  1020] loss: 0.222\n",
      "[16,  1030] loss: 0.185\n",
      "[16,  1040] loss: 0.268\n",
      "[16,  1050] loss: 0.226\n",
      "[16,  1060] loss: 0.181\n",
      "[16,  1070] loss: 0.212\n",
      "[16,  1080] loss: 0.215\n",
      "[16,  1090] loss: 0.167\n",
      "[16,  1100] loss: 0.180\n",
      "[16,  1110] loss: 0.215\n",
      "[16,  1120] loss: 0.209\n",
      "[16,  1130] loss: 0.182\n",
      "[16,  1140] loss: 0.193\n",
      "[16,  1150] loss: 0.186\n",
      "[16,  1160] loss: 0.218\n",
      "[16,  1170] loss: 0.280\n",
      "[16,  1180] loss: 0.222\n",
      "[16,  1190] loss: 0.225\n",
      "[16,  1200] loss: 0.222\n",
      "Learning rate: 0.001000\n",
      "[17,    10] loss: 0.184\n",
      "[17,    20] loss: 0.250\n",
      "[17,    30] loss: 0.157\n",
      "[17,    40] loss: 0.202\n",
      "[17,    50] loss: 0.206\n",
      "[17,    60] loss: 0.213\n",
      "[17,    70] loss: 0.204\n",
      "[17,    80] loss: 0.211\n",
      "[17,    90] loss: 0.186\n",
      "[17,   100] loss: 0.209\n",
      "[17,   110] loss: 0.223\n",
      "[17,   120] loss: 0.173\n",
      "[17,   130] loss: 0.175\n",
      "[17,   140] loss: 0.224\n",
      "[17,   150] loss: 0.188\n",
      "[17,   160] loss: 0.169\n",
      "[17,   170] loss: 0.199\n",
      "[17,   180] loss: 0.160\n",
      "[17,   190] loss: 0.178\n",
      "[17,   200] loss: 0.189\n",
      "[17,   210] loss: 0.205\n",
      "[17,   220] loss: 0.155\n",
      "[17,   230] loss: 0.179\n",
      "[17,   240] loss: 0.170\n",
      "[17,   250] loss: 0.178\n",
      "[17,   260] loss: 0.207\n",
      "[17,   270] loss: 0.220\n",
      "[17,   280] loss: 0.155\n",
      "[17,   290] loss: 0.192\n",
      "[17,   300] loss: 0.215\n",
      "[17,   310] loss: 0.170\n",
      "[17,   320] loss: 0.245\n",
      "[17,   330] loss: 0.171\n",
      "[17,   340] loss: 0.211\n",
      "[17,   350] loss: 0.169\n",
      "[17,   360] loss: 0.191\n",
      "[17,   370] loss: 0.178\n",
      "[17,   380] loss: 0.180\n",
      "[17,   390] loss: 0.203\n",
      "[17,   400] loss: 0.217\n",
      "[17,   410] loss: 0.175\n",
      "[17,   420] loss: 0.175\n",
      "[17,   430] loss: 0.185\n",
      "[17,   440] loss: 0.168\n",
      "[17,   450] loss: 0.211\n",
      "[17,   460] loss: 0.160\n",
      "[17,   470] loss: 0.226\n",
      "[17,   480] loss: 0.175\n",
      "[17,   490] loss: 0.165\n",
      "[17,   500] loss: 0.160\n",
      "[17,   510] loss: 0.225\n",
      "[17,   520] loss: 0.242\n",
      "[17,   530] loss: 0.187\n",
      "[17,   540] loss: 0.209\n",
      "[17,   550] loss: 0.189\n",
      "[17,   560] loss: 0.177\n",
      "[17,   570] loss: 0.142\n",
      "[17,   580] loss: 0.198\n",
      "[17,   590] loss: 0.183\n",
      "[17,   600] loss: 0.156\n",
      "[17,   610] loss: 0.220\n",
      "[17,   620] loss: 0.203\n",
      "[17,   630] loss: 0.237\n",
      "[17,   640] loss: 0.276\n",
      "[17,   650] loss: 0.148\n",
      "[17,   660] loss: 0.241\n",
      "[17,   670] loss: 0.190\n",
      "[17,   680] loss: 0.210\n",
      "[17,   690] loss: 0.202\n",
      "[17,   700] loss: 0.163\n",
      "[17,   710] loss: 0.192\n",
      "[17,   720] loss: 0.166\n",
      "[17,   730] loss: 0.170\n",
      "[17,   740] loss: 0.221\n",
      "[17,   750] loss: 0.168\n",
      "[17,   760] loss: 0.212\n",
      "[17,   770] loss: 0.224\n",
      "[17,   780] loss: 0.161\n",
      "[17,   790] loss: 0.173\n",
      "[17,   800] loss: 0.212\n",
      "[17,   810] loss: 0.238\n",
      "[17,   820] loss: 0.189\n",
      "[17,   830] loss: 0.242\n",
      "[17,   840] loss: 0.193\n",
      "[17,   850] loss: 0.162\n",
      "[17,   860] loss: 0.237\n",
      "[17,   870] loss: 0.185\n",
      "[17,   880] loss: 0.166\n",
      "[17,   890] loss: 0.259\n",
      "[17,   900] loss: 0.223\n",
      "[17,   910] loss: 0.192\n",
      "[17,   920] loss: 0.160\n",
      "[17,   930] loss: 0.203\n",
      "[17,   940] loss: 0.172\n",
      "[17,   950] loss: 0.205\n",
      "[17,   960] loss: 0.180\n",
      "[17,   970] loss: 0.179\n",
      "[17,   980] loss: 0.200\n",
      "[17,   990] loss: 0.217\n",
      "[17,  1000] loss: 0.206\n",
      "[17,  1010] loss: 0.199\n",
      "[17,  1020] loss: 0.164\n",
      "[17,  1030] loss: 0.168\n",
      "[17,  1040] loss: 0.221\n",
      "[17,  1050] loss: 0.166\n",
      "[17,  1060] loss: 0.142\n",
      "[17,  1070] loss: 0.202\n",
      "[17,  1080] loss: 0.206\n",
      "[17,  1090] loss: 0.137\n",
      "[17,  1100] loss: 0.208\n",
      "[17,  1110] loss: 0.189\n",
      "[17,  1120] loss: 0.196\n",
      "[17,  1130] loss: 0.246\n",
      "[17,  1140] loss: 0.216\n",
      "[17,  1150] loss: 0.157\n",
      "[17,  1160] loss: 0.191\n",
      "[17,  1170] loss: 0.185\n",
      "[17,  1180] loss: 0.173\n",
      "[17,  1190] loss: 0.159\n",
      "[17,  1200] loss: 0.150\n",
      "Learning rate: 0.001000\n",
      "[18,    10] loss: 0.161\n",
      "[18,    20] loss: 0.126\n",
      "[18,    30] loss: 0.166\n",
      "[18,    40] loss: 0.185\n",
      "[18,    50] loss: 0.171\n",
      "[18,    60] loss: 0.202\n",
      "[18,    70] loss: 0.150\n",
      "[18,    80] loss: 0.165\n",
      "[18,    90] loss: 0.143\n",
      "[18,   100] loss: 0.170\n",
      "[18,   110] loss: 0.148\n",
      "[18,   120] loss: 0.170\n",
      "[18,   130] loss: 0.151\n",
      "[18,   140] loss: 0.163\n",
      "[18,   150] loss: 0.153\n",
      "[18,   160] loss: 0.203\n",
      "[18,   170] loss: 0.179\n",
      "[18,   180] loss: 0.175\n",
      "[18,   190] loss: 0.197\n",
      "[18,   200] loss: 0.212\n",
      "[18,   210] loss: 0.151\n",
      "[18,   220] loss: 0.148\n",
      "[18,   230] loss: 0.206\n",
      "[18,   240] loss: 0.170\n",
      "[18,   250] loss: 0.174\n",
      "[18,   260] loss: 0.177\n",
      "[18,   270] loss: 0.137\n",
      "[18,   280] loss: 0.170\n",
      "[18,   290] loss: 0.138\n",
      "[18,   300] loss: 0.144\n",
      "[18,   310] loss: 0.173\n",
      "[18,   320] loss: 0.146\n",
      "[18,   330] loss: 0.201\n",
      "[18,   340] loss: 0.198\n",
      "[18,   350] loss: 0.157\n",
      "[18,   360] loss: 0.172\n",
      "[18,   370] loss: 0.182\n",
      "[18,   380] loss: 0.173\n",
      "[18,   390] loss: 0.167\n",
      "[18,   400] loss: 0.188\n",
      "[18,   410] loss: 0.146\n",
      "[18,   420] loss: 0.200\n",
      "[18,   430] loss: 0.185\n",
      "[18,   440] loss: 0.175\n",
      "[18,   450] loss: 0.155\n",
      "[18,   460] loss: 0.167\n",
      "[18,   470] loss: 0.181\n",
      "[18,   480] loss: 0.287\n",
      "[18,   490] loss: 0.156\n",
      "[18,   500] loss: 0.183\n",
      "[18,   510] loss: 0.215\n",
      "[18,   520] loss: 0.141\n",
      "[18,   530] loss: 0.176\n",
      "[18,   540] loss: 0.183\n",
      "[18,   550] loss: 0.184\n",
      "[18,   560] loss: 0.166\n",
      "[18,   570] loss: 0.161\n",
      "[18,   580] loss: 0.182\n",
      "[18,   590] loss: 0.198\n",
      "[18,   600] loss: 0.212\n",
      "[18,   610] loss: 0.193\n",
      "[18,   620] loss: 0.199\n",
      "[18,   630] loss: 0.167\n",
      "[18,   640] loss: 0.183\n",
      "[18,   650] loss: 0.138\n",
      "[18,   660] loss: 0.168\n",
      "[18,   670] loss: 0.228\n",
      "[18,   680] loss: 0.201\n",
      "[18,   690] loss: 0.206\n",
      "[18,   700] loss: 0.178\n",
      "[18,   710] loss: 0.172\n",
      "[18,   720] loss: 0.167\n",
      "[18,   730] loss: 0.177\n",
      "[18,   740] loss: 0.145\n",
      "[18,   750] loss: 0.165\n",
      "[18,   760] loss: 0.173\n",
      "[18,   770] loss: 0.142\n",
      "[18,   780] loss: 0.161\n",
      "[18,   790] loss: 0.159\n",
      "[18,   800] loss: 0.197\n",
      "[18,   810] loss: 0.176\n",
      "[18,   820] loss: 0.208\n",
      "[18,   830] loss: 0.212\n",
      "[18,   840] loss: 0.134\n",
      "[18,   850] loss: 0.187\n",
      "[18,   860] loss: 0.144\n",
      "[18,   870] loss: 0.158\n",
      "[18,   880] loss: 0.223\n",
      "[18,   890] loss: 0.138\n",
      "[18,   900] loss: 0.159\n",
      "[18,   910] loss: 0.180\n",
      "[18,   920] loss: 0.161\n",
      "[18,   930] loss: 0.144\n",
      "[18,   940] loss: 0.186\n",
      "[18,   950] loss: 0.178\n",
      "[18,   960] loss: 0.141\n",
      "[18,   970] loss: 0.172\n",
      "[18,   980] loss: 0.168\n",
      "[18,   990] loss: 0.189\n",
      "[18,  1000] loss: 0.187\n",
      "[18,  1010] loss: 0.142\n",
      "[18,  1020] loss: 0.148\n",
      "[18,  1030] loss: 0.196\n",
      "[18,  1040] loss: 0.180\n",
      "[18,  1050] loss: 0.208\n",
      "[18,  1060] loss: 0.146\n",
      "[18,  1070] loss: 0.180\n",
      "[18,  1080] loss: 0.198\n",
      "[18,  1090] loss: 0.247\n",
      "[18,  1100] loss: 0.183\n",
      "[18,  1110] loss: 0.151\n",
      "[18,  1120] loss: 0.138\n",
      "[18,  1130] loss: 0.153\n",
      "[18,  1140] loss: 0.167\n",
      "[18,  1150] loss: 0.114\n",
      "[18,  1160] loss: 0.162\n",
      "[18,  1170] loss: 0.155\n",
      "[18,  1180] loss: 0.176\n",
      "[18,  1190] loss: 0.181\n",
      "[18,  1200] loss: 0.187\n",
      "Learning rate: 0.000100\n",
      "[19,    10] loss: 0.136\n",
      "[19,    20] loss: 0.200\n",
      "[19,    30] loss: 0.167\n",
      "[19,    40] loss: 0.165\n",
      "[19,    50] loss: 0.144\n",
      "[19,    60] loss: 0.172\n",
      "[19,    70] loss: 0.181\n",
      "[19,    80] loss: 0.133\n",
      "[19,    90] loss: 0.135\n",
      "[19,   100] loss: 0.149\n",
      "[19,   110] loss: 0.220\n",
      "[19,   120] loss: 0.123\n",
      "[19,   130] loss: 0.160\n",
      "[19,   140] loss: 0.192\n",
      "[19,   150] loss: 0.195\n",
      "[19,   160] loss: 0.202\n",
      "[19,   170] loss: 0.160\n",
      "[19,   180] loss: 0.133\n",
      "[19,   190] loss: 0.139\n",
      "[19,   200] loss: 0.142\n",
      "[19,   210] loss: 0.154\n",
      "[19,   220] loss: 0.141\n",
      "[19,   230] loss: 0.119\n",
      "[19,   240] loss: 0.157\n",
      "[19,   250] loss: 0.157\n",
      "[19,   260] loss: 0.156\n",
      "[19,   270] loss: 0.170\n",
      "[19,   280] loss: 0.171\n",
      "[19,   290] loss: 0.129\n",
      "[19,   300] loss: 0.162\n",
      "[19,   310] loss: 0.143\n",
      "[19,   320] loss: 0.164\n",
      "[19,   330] loss: 0.183\n",
      "[19,   340] loss: 0.111\n",
      "[19,   350] loss: 0.170\n",
      "[19,   360] loss: 0.162\n",
      "[19,   370] loss: 0.132\n",
      "[19,   380] loss: 0.177\n",
      "[19,   390] loss: 0.188\n",
      "[19,   400] loss: 0.151\n",
      "[19,   410] loss: 0.168\n",
      "[19,   420] loss: 0.191\n",
      "[19,   430] loss: 0.181\n",
      "[19,   440] loss: 0.104\n",
      "[19,   450] loss: 0.164\n",
      "[19,   460] loss: 0.124\n",
      "[19,   470] loss: 0.127\n",
      "[19,   480] loss: 0.168\n",
      "[19,   490] loss: 0.098\n",
      "[19,   500] loss: 0.162\n",
      "[19,   510] loss: 0.148\n",
      "[19,   520] loss: 0.130\n",
      "[19,   530] loss: 0.199\n",
      "[19,   540] loss: 0.118\n",
      "[19,   550] loss: 0.156\n",
      "[19,   560] loss: 0.158\n",
      "[19,   570] loss: 0.172\n",
      "[19,   580] loss: 0.131\n",
      "[19,   590] loss: 0.174\n",
      "[19,   600] loss: 0.116\n",
      "[19,   610] loss: 0.184\n",
      "[19,   620] loss: 0.115\n",
      "[19,   630] loss: 0.148\n",
      "[19,   640] loss: 0.129\n",
      "[19,   650] loss: 0.175\n",
      "[19,   660] loss: 0.150\n",
      "[19,   670] loss: 0.177\n",
      "[19,   680] loss: 0.191\n",
      "[19,   690] loss: 0.142\n",
      "[19,   700] loss: 0.189\n",
      "[19,   710] loss: 0.137\n",
      "[19,   720] loss: 0.193\n",
      "[19,   730] loss: 0.107\n",
      "[19,   740] loss: 0.157\n",
      "[19,   750] loss: 0.147\n",
      "[19,   760] loss: 0.121\n",
      "[19,   770] loss: 0.161\n",
      "[19,   780] loss: 0.132\n",
      "[19,   790] loss: 0.159\n",
      "[19,   800] loss: 0.118\n",
      "[19,   810] loss: 0.128\n",
      "[19,   820] loss: 0.147\n",
      "[19,   830] loss: 0.167\n",
      "[19,   840] loss: 0.133\n",
      "[19,   850] loss: 0.143\n",
      "[19,   860] loss: 0.119\n",
      "[19,   870] loss: 0.152\n",
      "[19,   880] loss: 0.145\n",
      "[19,   890] loss: 0.167\n",
      "[19,   900] loss: 0.136\n",
      "[19,   910] loss: 0.157\n",
      "[19,   920] loss: 0.157\n",
      "[19,   930] loss: 0.112\n",
      "[19,   940] loss: 0.131\n",
      "[19,   950] loss: 0.169\n",
      "[19,   960] loss: 0.140\n",
      "[19,   970] loss: 0.119\n",
      "[19,   980] loss: 0.138\n",
      "[19,   990] loss: 0.154\n",
      "[19,  1000] loss: 0.153\n",
      "[19,  1010] loss: 0.174\n",
      "[19,  1020] loss: 0.149\n",
      "[19,  1030] loss: 0.162\n",
      "[19,  1040] loss: 0.147\n",
      "[19,  1050] loss: 0.181\n",
      "[19,  1060] loss: 0.163\n",
      "[19,  1070] loss: 0.175\n",
      "[19,  1080] loss: 0.141\n",
      "[19,  1090] loss: 0.143\n",
      "[19,  1100] loss: 0.153\n",
      "[19,  1110] loss: 0.184\n",
      "[19,  1120] loss: 0.146\n",
      "[19,  1130] loss: 0.152\n",
      "[19,  1140] loss: 0.113\n",
      "[19,  1150] loss: 0.151\n",
      "[19,  1160] loss: 0.162\n",
      "[19,  1170] loss: 0.149\n",
      "[19,  1180] loss: 0.124\n",
      "[19,  1190] loss: 0.107\n",
      "[19,  1200] loss: 0.145\n",
      "Learning rate: 0.000100\n",
      "[20,    10] loss: 0.140\n",
      "[20,    20] loss: 0.142\n",
      "[20,    30] loss: 0.149\n",
      "[20,    40] loss: 0.172\n",
      "[20,    50] loss: 0.195\n",
      "[20,    60] loss: 0.168\n",
      "[20,    70] loss: 0.186\n",
      "[20,    80] loss: 0.142\n",
      "[20,    90] loss: 0.181\n",
      "[20,   100] loss: 0.143\n",
      "[20,   110] loss: 0.142\n",
      "[20,   120] loss: 0.167\n",
      "[20,   130] loss: 0.210\n",
      "[20,   140] loss: 0.195\n",
      "[20,   150] loss: 0.145\n",
      "[20,   160] loss: 0.140\n",
      "[20,   170] loss: 0.137\n",
      "[20,   180] loss: 0.174\n",
      "[20,   190] loss: 0.132\n",
      "[20,   200] loss: 0.162\n",
      "[20,   210] loss: 0.174\n",
      "[20,   220] loss: 0.127\n",
      "[20,   230] loss: 0.115\n",
      "[20,   240] loss: 0.138\n",
      "[20,   250] loss: 0.163\n",
      "[20,   260] loss: 0.121\n",
      "[20,   270] loss: 0.114\n",
      "[20,   280] loss: 0.136\n",
      "[20,   290] loss: 0.153\n",
      "[20,   300] loss: 0.160\n",
      "[20,   310] loss: 0.166\n",
      "[20,   320] loss: 0.136\n",
      "[20,   330] loss: 0.168\n",
      "[20,   340] loss: 0.167\n",
      "[20,   350] loss: 0.134\n",
      "[20,   360] loss: 0.147\n",
      "[20,   370] loss: 0.148\n",
      "[20,   380] loss: 0.162\n",
      "[20,   390] loss: 0.116\n",
      "[20,   400] loss: 0.153\n",
      "[20,   410] loss: 0.177\n",
      "[20,   420] loss: 0.121\n",
      "[20,   430] loss: 0.110\n",
      "[20,   440] loss: 0.124\n",
      "[20,   450] loss: 0.150\n",
      "[20,   460] loss: 0.131\n",
      "[20,   470] loss: 0.168\n",
      "[20,   480] loss: 0.142\n",
      "[20,   490] loss: 0.124\n",
      "[20,   500] loss: 0.133\n",
      "[20,   510] loss: 0.195\n",
      "[20,   520] loss: 0.145\n",
      "[20,   530] loss: 0.164\n",
      "[20,   540] loss: 0.140\n",
      "[20,   550] loss: 0.185\n",
      "[20,   560] loss: 0.124\n",
      "[20,   570] loss: 0.110\n",
      "[20,   580] loss: 0.174\n",
      "[20,   590] loss: 0.127\n",
      "[20,   600] loss: 0.146\n",
      "[20,   610] loss: 0.194\n",
      "[20,   620] loss: 0.139\n",
      "[20,   630] loss: 0.155\n",
      "[20,   640] loss: 0.138\n",
      "[20,   650] loss: 0.100\n",
      "[20,   660] loss: 0.111\n",
      "[20,   670] loss: 0.174\n",
      "[20,   680] loss: 0.180\n",
      "[20,   690] loss: 0.134\n",
      "[20,   700] loss: 0.143\n",
      "[20,   710] loss: 0.137\n",
      "[20,   720] loss: 0.200\n",
      "[20,   730] loss: 0.126\n",
      "[20,   740] loss: 0.169\n",
      "[20,   750] loss: 0.113\n",
      "[20,   760] loss: 0.134\n",
      "[20,   770] loss: 0.153\n",
      "[20,   780] loss: 0.127\n",
      "[20,   790] loss: 0.121\n",
      "[20,   800] loss: 0.146\n",
      "[20,   810] loss: 0.173\n",
      "[20,   820] loss: 0.140\n",
      "[20,   830] loss: 0.143\n",
      "[20,   840] loss: 0.140\n",
      "[20,   850] loss: 0.174\n",
      "[20,   860] loss: 0.136\n",
      "[20,   870] loss: 0.125\n",
      "[20,   880] loss: 0.120\n",
      "[20,   890] loss: 0.140\n",
      "[20,   900] loss: 0.168\n",
      "[20,   910] loss: 0.135\n",
      "[20,   920] loss: 0.122\n",
      "[20,   930] loss: 0.193\n",
      "[20,   940] loss: 0.152\n",
      "[20,   950] loss: 0.150\n",
      "[20,   960] loss: 0.149\n",
      "[20,   970] loss: 0.180\n",
      "[20,   980] loss: 0.153\n",
      "[20,   990] loss: 0.156\n",
      "[20,  1000] loss: 0.168\n",
      "[20,  1010] loss: 0.110\n",
      "[20,  1020] loss: 0.140\n",
      "[20,  1030] loss: 0.160\n",
      "[20,  1040] loss: 0.159\n",
      "[20,  1050] loss: 0.161\n",
      "[20,  1060] loss: 0.145\n",
      "[20,  1070] loss: 0.165\n",
      "[20,  1080] loss: 0.150\n",
      "[20,  1090] loss: 0.155\n",
      "[20,  1100] loss: 0.136\n",
      "[20,  1110] loss: 0.190\n",
      "[20,  1120] loss: 0.123\n",
      "[20,  1130] loss: 0.172\n",
      "[20,  1140] loss: 0.156\n",
      "[20,  1150] loss: 0.176\n",
      "[20,  1160] loss: 0.143\n",
      "[20,  1170] loss: 0.153\n",
      "[20,  1180] loss: 0.182\n",
      "[20,  1190] loss: 0.158\n",
      "[20,  1200] loss: 0.136\n",
      "Learning rate: 0.000100\n",
      "[21,    10] loss: 0.123\n",
      "[21,    20] loss: 0.125\n",
      "[21,    30] loss: 0.118\n",
      "[21,    40] loss: 0.149\n",
      "[21,    50] loss: 0.227\n",
      "[21,    60] loss: 0.133\n",
      "[21,    70] loss: 0.173\n",
      "[21,    80] loss: 0.104\n",
      "[21,    90] loss: 0.162\n",
      "[21,   100] loss: 0.130\n",
      "[21,   110] loss: 0.173\n",
      "[21,   120] loss: 0.165\n",
      "[21,   130] loss: 0.137\n",
      "[21,   140] loss: 0.126\n",
      "[21,   150] loss: 0.145\n",
      "[21,   160] loss: 0.160\n",
      "[21,   170] loss: 0.114\n",
      "[21,   180] loss: 0.168\n",
      "[21,   190] loss: 0.172\n",
      "[21,   200] loss: 0.160\n",
      "[21,   210] loss: 0.174\n",
      "[21,   220] loss: 0.141\n",
      "[21,   230] loss: 0.125\n",
      "[21,   240] loss: 0.144\n",
      "[21,   250] loss: 0.200\n",
      "[21,   260] loss: 0.112\n",
      "[21,   270] loss: 0.152\n",
      "[21,   280] loss: 0.150\n",
      "[21,   290] loss: 0.133\n",
      "[21,   300] loss: 0.142\n",
      "[21,   310] loss: 0.146\n",
      "[21,   320] loss: 0.152\n",
      "[21,   330] loss: 0.142\n",
      "[21,   340] loss: 0.129\n",
      "[21,   350] loss: 0.170\n",
      "[21,   360] loss: 0.168\n",
      "[21,   370] loss: 0.117\n",
      "[21,   380] loss: 0.147\n",
      "[21,   390] loss: 0.123\n",
      "[21,   400] loss: 0.167\n",
      "[21,   410] loss: 0.160\n",
      "[21,   420] loss: 0.177\n",
      "[21,   430] loss: 0.130\n",
      "[21,   440] loss: 0.149\n",
      "[21,   450] loss: 0.119\n",
      "[21,   460] loss: 0.139\n",
      "[21,   470] loss: 0.161\n",
      "[21,   480] loss: 0.161\n",
      "[21,   490] loss: 0.190\n",
      "[21,   500] loss: 0.119\n",
      "[21,   510] loss: 0.140\n",
      "[21,   520] loss: 0.172\n",
      "[21,   530] loss: 0.110\n",
      "[21,   540] loss: 0.142\n",
      "[21,   550] loss: 0.152\n",
      "[21,   560] loss: 0.163\n",
      "[21,   570] loss: 0.138\n",
      "[21,   580] loss: 0.139\n",
      "[21,   590] loss: 0.177\n",
      "[21,   600] loss: 0.171\n",
      "[21,   610] loss: 0.154\n",
      "[21,   620] loss: 0.204\n",
      "[21,   630] loss: 0.175\n",
      "[21,   640] loss: 0.164\n",
      "[21,   650] loss: 0.147\n",
      "[21,   660] loss: 0.145\n",
      "[21,   670] loss: 0.144\n",
      "[21,   680] loss: 0.145\n",
      "[21,   690] loss: 0.145\n",
      "[21,   700] loss: 0.168\n",
      "[21,   710] loss: 0.144\n",
      "[21,   720] loss: 0.113\n",
      "[21,   730] loss: 0.107\n",
      "[21,   740] loss: 0.171\n",
      "[21,   750] loss: 0.164\n",
      "[21,   760] loss: 0.127\n",
      "[21,   770] loss: 0.148\n",
      "[21,   780] loss: 0.127\n",
      "[21,   790] loss: 0.150\n",
      "[21,   800] loss: 0.135\n",
      "[21,   810] loss: 0.152\n",
      "[21,   820] loss: 0.151\n",
      "[21,   830] loss: 0.138\n",
      "[21,   840] loss: 0.157\n",
      "[21,   850] loss: 0.134\n",
      "[21,   860] loss: 0.133\n",
      "[21,   870] loss: 0.158\n",
      "[21,   880] loss: 0.180\n",
      "[21,   890] loss: 0.137\n",
      "[21,   900] loss: 0.165\n",
      "[21,   910] loss: 0.202\n",
      "[21,   920] loss: 0.140\n",
      "[21,   930] loss: 0.122\n",
      "[21,   940] loss: 0.136\n",
      "[21,   950] loss: 0.166\n",
      "[21,   960] loss: 0.170\n",
      "[21,   970] loss: 0.166\n",
      "[21,   980] loss: 0.142\n",
      "[21,   990] loss: 0.133\n",
      "[21,  1000] loss: 0.167\n",
      "[21,  1010] loss: 0.163\n",
      "[21,  1020] loss: 0.131\n",
      "[21,  1030] loss: 0.149\n",
      "[21,  1040] loss: 0.168\n",
      "[21,  1050] loss: 0.108\n",
      "[21,  1060] loss: 0.139\n",
      "[21,  1070] loss: 0.171\n",
      "[21,  1080] loss: 0.225\n",
      "[21,  1090] loss: 0.171\n",
      "[21,  1100] loss: 0.184\n",
      "[21,  1110] loss: 0.123\n",
      "[21,  1120] loss: 0.134\n",
      "[21,  1130] loss: 0.166\n",
      "[21,  1140] loss: 0.151\n",
      "[21,  1150] loss: 0.168\n",
      "[21,  1160] loss: 0.129\n",
      "[21,  1170] loss: 0.163\n",
      "[21,  1180] loss: 0.132\n",
      "[21,  1190] loss: 0.139\n",
      "[21,  1200] loss: 0.135\n",
      "Learning rate: 0.000100\n",
      "[22,    10] loss: 0.163\n",
      "[22,    20] loss: 0.109\n",
      "[22,    30] loss: 0.176\n",
      "[22,    40] loss: 0.147\n",
      "[22,    50] loss: 0.183\n",
      "[22,    60] loss: 0.154\n",
      "[22,    70] loss: 0.133\n",
      "[22,    80] loss: 0.131\n",
      "[22,    90] loss: 0.116\n",
      "[22,   100] loss: 0.145\n",
      "[22,   110] loss: 0.133\n",
      "[22,   120] loss: 0.151\n",
      "[22,   130] loss: 0.128\n",
      "[22,   140] loss: 0.143\n",
      "[22,   150] loss: 0.138\n",
      "[22,   160] loss: 0.191\n",
      "[22,   170] loss: 0.126\n",
      "[22,   180] loss: 0.131\n",
      "[22,   190] loss: 0.161\n",
      "[22,   200] loss: 0.123\n",
      "[22,   210] loss: 0.154\n",
      "[22,   220] loss: 0.153\n",
      "[22,   230] loss: 0.126\n",
      "[22,   240] loss: 0.174\n",
      "[22,   250] loss: 0.117\n",
      "[22,   260] loss: 0.183\n",
      "[22,   270] loss: 0.101\n",
      "[22,   280] loss: 0.117\n",
      "[22,   290] loss: 0.124\n",
      "[22,   300] loss: 0.143\n",
      "[22,   310] loss: 0.125\n",
      "[22,   320] loss: 0.111\n",
      "[22,   330] loss: 0.131\n",
      "[22,   340] loss: 0.122\n",
      "[22,   350] loss: 0.144\n",
      "[22,   360] loss: 0.210\n",
      "[22,   370] loss: 0.156\n",
      "[22,   380] loss: 0.158\n",
      "[22,   390] loss: 0.206\n",
      "[22,   400] loss: 0.146\n",
      "[22,   410] loss: 0.143\n",
      "[22,   420] loss: 0.190\n",
      "[22,   430] loss: 0.167\n",
      "[22,   440] loss: 0.126\n",
      "[22,   450] loss: 0.121\n",
      "[22,   460] loss: 0.143\n",
      "[22,   470] loss: 0.111\n",
      "[22,   480] loss: 0.126\n",
      "[22,   490] loss: 0.116\n",
      "[22,   500] loss: 0.146\n",
      "[22,   510] loss: 0.123\n",
      "[22,   520] loss: 0.152\n",
      "[22,   530] loss: 0.165\n",
      "[22,   540] loss: 0.140\n",
      "[22,   550] loss: 0.195\n",
      "[22,   560] loss: 0.135\n",
      "[22,   570] loss: 0.171\n",
      "[22,   580] loss: 0.148\n",
      "[22,   590] loss: 0.140\n",
      "[22,   600] loss: 0.125\n",
      "[22,   610] loss: 0.150\n",
      "[22,   620] loss: 0.146\n",
      "[22,   630] loss: 0.155\n",
      "[22,   640] loss: 0.135\n",
      "[22,   650] loss: 0.152\n",
      "[22,   660] loss: 0.170\n",
      "[22,   670] loss: 0.160\n",
      "[22,   680] loss: 0.171\n",
      "[22,   690] loss: 0.104\n",
      "[22,   700] loss: 0.131\n",
      "[22,   710] loss: 0.141\n",
      "[22,   720] loss: 0.187\n",
      "[22,   730] loss: 0.147\n",
      "[22,   740] loss: 0.143\n",
      "[22,   750] loss: 0.124\n",
      "[22,   760] loss: 0.179\n",
      "[22,   770] loss: 0.172\n",
      "[22,   780] loss: 0.153\n",
      "[22,   790] loss: 0.176\n",
      "[22,   800] loss: 0.119\n",
      "[22,   810] loss: 0.114\n",
      "[22,   820] loss: 0.147\n",
      "[22,   830] loss: 0.116\n",
      "[22,   840] loss: 0.139\n",
      "[22,   850] loss: 0.139\n",
      "[22,   860] loss: 0.153\n",
      "[22,   870] loss: 0.135\n",
      "[22,   880] loss: 0.175\n",
      "[22,   890] loss: 0.139\n",
      "[22,   900] loss: 0.195\n",
      "[22,   910] loss: 0.119\n",
      "[22,   920] loss: 0.138\n",
      "[22,   930] loss: 0.126\n",
      "[22,   940] loss: 0.127\n",
      "[22,   950] loss: 0.175\n",
      "[22,   960] loss: 0.140\n",
      "[22,   970] loss: 0.130\n",
      "[22,   980] loss: 0.201\n",
      "[22,   990] loss: 0.139\n",
      "[22,  1000] loss: 0.132\n",
      "[22,  1010] loss: 0.154\n",
      "[22,  1020] loss: 0.147\n",
      "[22,  1030] loss: 0.153\n",
      "[22,  1040] loss: 0.144\n",
      "[22,  1050] loss: 0.146\n",
      "[22,  1060] loss: 0.120\n",
      "[22,  1070] loss: 0.131\n",
      "[22,  1080] loss: 0.113\n",
      "[22,  1090] loss: 0.138\n",
      "[22,  1100] loss: 0.158\n",
      "[22,  1110] loss: 0.161\n",
      "[22,  1120] loss: 0.137\n",
      "[22,  1130] loss: 0.135\n",
      "[22,  1140] loss: 0.145\n",
      "[22,  1150] loss: 0.115\n",
      "[22,  1160] loss: 0.127\n",
      "[22,  1170] loss: 0.181\n",
      "[22,  1180] loss: 0.131\n",
      "[22,  1190] loss: 0.177\n",
      "[22,  1200] loss: 0.145\n",
      "Learning rate: 0.000100\n",
      "[23,    10] loss: 0.126\n",
      "[23,    20] loss: 0.142\n",
      "[23,    30] loss: 0.142\n",
      "[23,    40] loss: 0.164\n",
      "[23,    50] loss: 0.151\n",
      "[23,    60] loss: 0.130\n",
      "[23,    70] loss: 0.178\n",
      "[23,    80] loss: 0.186\n",
      "[23,    90] loss: 0.126\n",
      "[23,   100] loss: 0.130\n",
      "[23,   110] loss: 0.114\n",
      "[23,   120] loss: 0.137\n",
      "[23,   130] loss: 0.161\n",
      "[23,   140] loss: 0.122\n",
      "[23,   150] loss: 0.158\n",
      "[23,   160] loss: 0.180\n",
      "[23,   170] loss: 0.137\n",
      "[23,   180] loss: 0.129\n",
      "[23,   190] loss: 0.121\n",
      "[23,   200] loss: 0.140\n",
      "[23,   210] loss: 0.134\n",
      "[23,   220] loss: 0.165\n",
      "[23,   230] loss: 0.143\n",
      "[23,   240] loss: 0.176\n",
      "[23,   250] loss: 0.113\n",
      "[23,   260] loss: 0.140\n",
      "[23,   270] loss: 0.128\n",
      "[23,   280] loss: 0.144\n",
      "[23,   290] loss: 0.130\n",
      "[23,   300] loss: 0.111\n",
      "[23,   310] loss: 0.154\n",
      "[23,   320] loss: 0.133\n",
      "[23,   330] loss: 0.188\n",
      "[23,   340] loss: 0.139\n",
      "[23,   350] loss: 0.132\n",
      "[23,   360] loss: 0.120\n",
      "[23,   370] loss: 0.157\n",
      "[23,   380] loss: 0.129\n",
      "[23,   390] loss: 0.166\n",
      "[23,   400] loss: 0.194\n",
      "[23,   410] loss: 0.111\n",
      "[23,   420] loss: 0.117\n",
      "[23,   430] loss: 0.151\n",
      "[23,   440] loss: 0.147\n",
      "[23,   450] loss: 0.138\n",
      "[23,   460] loss: 0.157\n",
      "[23,   470] loss: 0.115\n",
      "[23,   480] loss: 0.123\n",
      "[23,   490] loss: 0.124\n",
      "[23,   500] loss: 0.173\n",
      "[23,   510] loss: 0.147\n",
      "[23,   520] loss: 0.185\n",
      "[23,   530] loss: 0.193\n",
      "[23,   540] loss: 0.118\n",
      "[23,   550] loss: 0.095\n",
      "[23,   560] loss: 0.137\n",
      "[23,   570] loss: 0.154\n",
      "[23,   580] loss: 0.121\n",
      "[23,   590] loss: 0.142\n",
      "[23,   600] loss: 0.133\n",
      "[23,   610] loss: 0.175\n",
      "[23,   620] loss: 0.128\n",
      "[23,   630] loss: 0.146\n",
      "[23,   640] loss: 0.152\n",
      "[23,   650] loss: 0.122\n",
      "[23,   660] loss: 0.164\n",
      "[23,   670] loss: 0.148\n",
      "[23,   680] loss: 0.132\n",
      "[23,   690] loss: 0.139\n",
      "[23,   700] loss: 0.135\n",
      "[23,   710] loss: 0.155\n",
      "[23,   720] loss: 0.113\n",
      "[23,   730] loss: 0.159\n",
      "[23,   740] loss: 0.141\n",
      "[23,   750] loss: 0.124\n",
      "[23,   760] loss: 0.135\n",
      "[23,   770] loss: 0.108\n",
      "[23,   780] loss: 0.142\n",
      "[23,   790] loss: 0.121\n",
      "[23,   800] loss: 0.127\n",
      "[23,   810] loss: 0.130\n",
      "[23,   820] loss: 0.138\n",
      "[23,   830] loss: 0.143\n",
      "[23,   840] loss: 0.152\n",
      "[23,   850] loss: 0.135\n",
      "[23,   860] loss: 0.098\n",
      "[23,   870] loss: 0.131\n",
      "[23,   880] loss: 0.176\n",
      "[23,   890] loss: 0.182\n",
      "[23,   900] loss: 0.130\n",
      "[23,   910] loss: 0.130\n",
      "[23,   920] loss: 0.120\n",
      "[23,   930] loss: 0.135\n",
      "[23,   940] loss: 0.125\n",
      "[23,   950] loss: 0.127\n",
      "[23,   960] loss: 0.127\n",
      "[23,   970] loss: 0.146\n",
      "[23,   980] loss: 0.142\n",
      "[23,   990] loss: 0.150\n",
      "[23,  1000] loss: 0.155\n",
      "[23,  1010] loss: 0.116\n",
      "[23,  1020] loss: 0.175\n",
      "[23,  1030] loss: 0.147\n",
      "[23,  1040] loss: 0.174\n",
      "[23,  1050] loss: 0.191\n",
      "[23,  1060] loss: 0.195\n",
      "[23,  1070] loss: 0.134\n",
      "[23,  1080] loss: 0.154\n",
      "[23,  1090] loss: 0.140\n",
      "[23,  1100] loss: 0.142\n",
      "[23,  1110] loss: 0.134\n",
      "[23,  1120] loss: 0.130\n",
      "[23,  1130] loss: 0.114\n",
      "[23,  1140] loss: 0.126\n",
      "[23,  1150] loss: 0.121\n",
      "[23,  1160] loss: 0.165\n",
      "[23,  1170] loss: 0.150\n",
      "[23,  1180] loss: 0.158\n",
      "[23,  1190] loss: 0.157\n",
      "[23,  1200] loss: 0.166\n",
      "Learning rate: 0.000010\n",
      "[24,    10] loss: 0.131\n",
      "[24,    20] loss: 0.158\n",
      "[24,    30] loss: 0.148\n",
      "[24,    40] loss: 0.114\n",
      "[24,    50] loss: 0.142\n",
      "[24,    60] loss: 0.172\n",
      "[24,    70] loss: 0.146\n",
      "[24,    80] loss: 0.129\n",
      "[24,    90] loss: 0.145\n",
      "[24,   100] loss: 0.143\n",
      "[24,   110] loss: 0.110\n",
      "[24,   120] loss: 0.138\n",
      "[24,   130] loss: 0.144\n",
      "[24,   140] loss: 0.191\n",
      "[24,   150] loss: 0.127\n",
      "[24,   160] loss: 0.125\n",
      "[24,   170] loss: 0.122\n",
      "[24,   180] loss: 0.145\n",
      "[24,   190] loss: 0.134\n",
      "[24,   200] loss: 0.112\n",
      "[24,   210] loss: 0.146\n",
      "[24,   220] loss: 0.142\n",
      "[24,   230] loss: 0.147\n",
      "[24,   240] loss: 0.164\n",
      "[24,   250] loss: 0.129\n",
      "[24,   260] loss: 0.127\n",
      "[24,   270] loss: 0.111\n",
      "[24,   280] loss: 0.190\n",
      "[24,   290] loss: 0.143\n",
      "[24,   300] loss: 0.129\n",
      "[24,   310] loss: 0.125\n",
      "[24,   320] loss: 0.115\n",
      "[24,   330] loss: 0.115\n",
      "[24,   340] loss: 0.162\n",
      "[24,   350] loss: 0.123\n",
      "[24,   360] loss: 0.159\n",
      "[24,   370] loss: 0.153\n",
      "[24,   380] loss: 0.124\n",
      "[24,   390] loss: 0.157\n",
      "[24,   400] loss: 0.116\n",
      "[24,   410] loss: 0.153\n",
      "[24,   420] loss: 0.190\n",
      "[24,   430] loss: 0.150\n",
      "[24,   440] loss: 0.177\n",
      "[24,   450] loss: 0.148\n",
      "[24,   460] loss: 0.135\n",
      "[24,   470] loss: 0.147\n",
      "[24,   480] loss: 0.109\n",
      "[24,   490] loss: 0.115\n",
      "[24,   500] loss: 0.109\n",
      "[24,   510] loss: 0.140\n",
      "[24,   520] loss: 0.139\n",
      "[24,   530] loss: 0.152\n",
      "[24,   540] loss: 0.176\n",
      "[24,   550] loss: 0.153\n",
      "[24,   560] loss: 0.123\n",
      "[24,   570] loss: 0.173\n",
      "[24,   580] loss: 0.141\n",
      "[24,   590] loss: 0.146\n",
      "[24,   600] loss: 0.130\n",
      "[24,   610] loss: 0.110\n",
      "[24,   620] loss: 0.128\n",
      "[24,   630] loss: 0.141\n",
      "[24,   640] loss: 0.139\n",
      "[24,   650] loss: 0.116\n",
      "[24,   660] loss: 0.107\n",
      "[24,   670] loss: 0.141\n",
      "[24,   680] loss: 0.125\n",
      "[24,   690] loss: 0.142\n",
      "[24,   700] loss: 0.140\n",
      "[24,   710] loss: 0.138\n",
      "[24,   720] loss: 0.117\n",
      "[24,   730] loss: 0.140\n",
      "[24,   740] loss: 0.152\n",
      "[24,   750] loss: 0.132\n",
      "[24,   760] loss: 0.174\n",
      "[24,   770] loss: 0.138\n",
      "[24,   780] loss: 0.140\n",
      "[24,   790] loss: 0.164\n",
      "[24,   800] loss: 0.160\n",
      "[24,   810] loss: 0.166\n",
      "[24,   820] loss: 0.128\n",
      "[24,   830] loss: 0.115\n",
      "[24,   840] loss: 0.127\n",
      "[24,   850] loss: 0.174\n",
      "[24,   860] loss: 0.118\n",
      "[24,   870] loss: 0.126\n",
      "[24,   880] loss: 0.135\n",
      "[24,   890] loss: 0.145\n",
      "[24,   900] loss: 0.133\n",
      "[24,   910] loss: 0.119\n",
      "[24,   920] loss: 0.129\n",
      "[24,   930] loss: 0.101\n",
      "[24,   940] loss: 0.115\n",
      "[24,   950] loss: 0.149\n",
      "[24,   960] loss: 0.134\n",
      "[24,   970] loss: 0.163\n",
      "[24,   980] loss: 0.179\n",
      "[24,   990] loss: 0.165\n",
      "[24,  1000] loss: 0.123\n",
      "[24,  1010] loss: 0.150\n",
      "[24,  1020] loss: 0.153\n",
      "[24,  1030] loss: 0.217\n",
      "[24,  1040] loss: 0.215\n",
      "[24,  1050] loss: 0.190\n",
      "[24,  1060] loss: 0.134\n",
      "[24,  1070] loss: 0.155\n",
      "[24,  1080] loss: 0.154\n",
      "[24,  1090] loss: 0.149\n",
      "[24,  1100] loss: 0.146\n",
      "[24,  1110] loss: 0.132\n",
      "[24,  1120] loss: 0.131\n",
      "[24,  1130] loss: 0.131\n",
      "[24,  1140] loss: 0.122\n",
      "[24,  1150] loss: 0.138\n",
      "[24,  1160] loss: 0.134\n",
      "[24,  1170] loss: 0.123\n",
      "[24,  1180] loss: 0.180\n",
      "[24,  1190] loss: 0.135\n",
      "[24,  1200] loss: 0.134\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import itertools\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "\n",
    "### set up directories\n",
    "prev_cpts = '/kaggle/input/bbbg_cpts/'\n",
    "checkpoints = '/kaggle/working/'\n",
    "\n",
    "def smooth(x, size):\n",
    "  return np.convolve(x, np.ones(size)/size, mode='valid')\n",
    "\n",
    "def get_bird_data(augmentation=0, input_size=128):\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.RandomCrop(input_size, padding=8, padding_mode='edge'), # Take 128x128 crops from padded images\n",
    "        transforms.RandomHorizontalFlip(),    # 50% of time flip image along y-axis\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    data_path = '/kaggle/input/birds23sp/birds/'\n",
    "\n",
    "    trainset = torchvision.datasets.ImageFolder(root=data_path + 'train', transform=transform_train)\n",
    "    testset = torchvision.datasets.ImageFolder(root=data_path + 'test', transform=transform_test)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=2)\n",
    "    \n",
    "    classes = open(data_path + \"names.txt\").read().strip().split(\"\\n\")\n",
    "    class_to_idx = trainset.class_to_idx\n",
    "    idx_to_class = {int(v): int(k) for k, v in class_to_idx.items()}\n",
    "    idx_to_name = {k: classes[v] for k,v in idx_to_class.items()}\n",
    "    return {'train': trainloader, 'test': testloader, 'to_class': idx_to_class, 'to_name':idx_to_name}\n",
    "\n",
    "def train(net, dataloader, epochs=1, start_epoch=0, lr=0.01, momentum=0.90, decay=0.0005, \n",
    "          verbose=1, print_every=10, state=None, checkpoint_path=None):\n",
    "    net.to(device)\n",
    "    net.train()\n",
    "    losses = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=decay)\n",
    "    #optimizer = optim.RMSprop(net.parameters(), lr=lr, momentum=momentum, weight_decay=decay)\n",
    "    #adam\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=2, threshold=0.0001, threshold_mode='abs')\n",
    "\n",
    "    # Load previous training state\n",
    "    if state:\n",
    "        net.load_state_dict(state['net'])\n",
    "        optimizer.load_state_dict(state['optimizer'])\n",
    "        start_epoch = state['epoch']\n",
    "        losses = state['losses']\n",
    "\n",
    "    # Fast forward lr schedule through already trained epochs\n",
    "    for epoch in range(start_epoch):\n",
    "        if epoch in schedule:\n",
    "            print (\"Learning rate: %f\"% schedule[epoch])\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = schedule[epoch]\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        sum_loss = 0.0\n",
    "        \n",
    "        for g in optimizer.param_groups:\n",
    "                print (\"Learning rate: %f\"% g['lr'])\n",
    "\n",
    "        # Update learning rate when scheduled\n",
    "#         if epoch in schedule:\n",
    "#             print (\"Learning rate: %f\"% schedule[epoch])\n",
    "#             for g in optimizer.param_groups:\n",
    "#                 g['lr'] = schedule[epoch]\n",
    "\n",
    "        for i, batch in enumerate(dataloader, 0):\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()  # autograd magic, computes all the partial derivatives\n",
    "            optimizer.step() # takes a step in gradient direction\n",
    "            \n",
    "\n",
    "            losses.append(loss.item())\n",
    "            sum_loss += loss.item()\n",
    "\n",
    "            if i % print_every == print_every-1:    # print every 10 mini-batches\n",
    "                if verbose:\n",
    "                  print('[%d, %5d] loss: %.3f' % (epoch, i + 1, sum_loss / print_every))\n",
    "                sum_loss = 0.0\n",
    "        scheduler.step(sum_loss)\n",
    "        if checkpoint_path:\n",
    "            state = {'epoch': epoch+1, 'net': net.state_dict(), 'optimizer': optimizer.state_dict(), 'losses': losses}\n",
    "            torch.save(state, checkpoint_path + 'resnet34-checkpoint-%d.pkl'%(epoch+1))\n",
    "            plt.plot(smooth(state['losses'], 50))\n",
    "            plt.savefig('checkpoint-%d.png'%(epoch+1))\n",
    "    return losses\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "data = get_bird_data(input_size=256)\n",
    "resnet = torch.hub.load('pytorch/vision:v0.6.0', 'resnet34', weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "resnet.fc = nn.Linear(512, 555)\n",
    "\n",
    "# selectable between starting at a checkpoint or from scratch.\n",
    "if (0):\n",
    "    state = torch.load(prev_cpts + 'checkpoint-15.pkl')\n",
    "    resnet.load_state_dict(state['net'])\n",
    "    losses = train(resnet, data['train'], epochs=20, lr=.0001, print_every=10, checkpoint_path=checkpoints, state=state)\n",
    "else: \n",
    "    losses = train(resnet, data['train'], epochs=25, lr=.01, print_every=10, checkpoint_path=checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82f9dc04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-05T00:31:41.113692Z",
     "iopub.status.busy": "2023-06-05T00:31:41.112802Z",
     "iopub.status.idle": "2023-06-05T00:31:42.018210Z",
     "shell.execute_reply": "2023-06-05T00:31:42.015995Z"
    },
    "papermill": {
     "duration": 1.135795,
     "end_time": "2023-06-05T00:31:42.019915",
     "exception": true,
     "start_time": "2023-06-05T00:31:40.884120",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/bbbg_cpts/resnet34-checkpoint-25.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m     out\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 21\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_cpts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresnet34-checkpoint-25.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     resnet\u001b[38;5;241m.\u001b[39mload_state_dict(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnet\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     23\u001b[0m     predict(resnet, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m], checkpoints \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreds_resnet34.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/bbbg_cpts/resnet34-checkpoint-25.pkl'"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict(net, dataloader, ofname):\n",
    "    out = open(ofname, 'w')\n",
    "    out.write(\"path,class\\n\")\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(dataloader, 0):\n",
    "            if i%100 == 0:\n",
    "                print(i)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            fname, _ = dataloader.dataset.samples[i]\n",
    "            out.write(\"test/{},{}\\n\".format(fname.split('/')[-1], data['to_class'][predicted.item()]))\n",
    "    out.close()\n",
    "\n",
    "\n",
    "if (1):\n",
    "    state = torch.load(prev_cpts + 'resnet34-checkpoint-25.pkl')\n",
    "    resnet.load_state_dict(state['net'])\n",
    "    predict(resnet, data['test'], checkpoints + \"preds_resnet34.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12055.537009,
   "end_time": "2023-06-05T00:31:45.313680",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-04T21:10:49.776671",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
